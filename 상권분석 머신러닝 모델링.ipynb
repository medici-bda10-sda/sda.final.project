{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 모델 검증 결과\n",
      "MAPE (평균 절대 퍼센트 오차): 0.0000\n",
      "RMSE (평균 제곱근 오차): 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m predicted_ratio \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future_X)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 16. 2024년 3분기 자치구 매출 예측 후 상권별 적용\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m district_total_future_sales \u001b[38;5;241m=\u001b[39m district_sales_grouped[district_sales_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m기준_년분기\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m future_quarter][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m자치구_총매출\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m predicted_sales \u001b[38;5;241m=\u001b[39m district_total_future_sales \u001b[38;5;241m*\u001b[39m predicted_ratio\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 선택한 상권: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_district\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "# 데이터 경로 설정\n",
    "base_path = \"C:/Users/m/Desktop/머신러닝 사용 데이터\"\n",
    "sales_data_path = f\"{base_path}/구 데이터/피처엔지니어링한통합데이터/피처엔지니어링일단다한통합데이터.csv\"\n",
    "pop_data_path = f\"{base_path}/서울 일별 유동인구.csv\"\n",
    "econ_index_path = f\"{base_path}/전처리 데이터/경제심리지수.csv\"\n",
    "cpi_data_path = f\"{base_path}/전처리 데이터/소비자물가지수.csv\"\n",
    "\n",
    "# 1. 데이터 로드\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "# 기준_년분기_코드 변환 (20191 → 2019Q1)\n",
    "sales_df[\"기준_년분기\"] = sales_df[\"기준_년분기_코드\"].astype(str).apply(lambda x: f\"{x[:4]}Q{int(x[4])}\")\n",
    "\n",
    "# 사용자 입력값 받기 (올바른 값이 입력될 때까지 반복)\n",
    "while True:\n",
    "    selected_district = input(\"예측할 상권명을 입력하세요: \").strip()\n",
    "    selected_industry = input(\"예측할 업종명을 입력하세요: \").strip()\n",
    "\n",
    "    # 3. 선택한 상권-업종 데이터 필터링\n",
    "    filtered_sales = sales_df[(sales_df[\"상권_코드_명\"] == selected_district) &\n",
    "                              (sales_df[\"서비스_업종_코드_명\"] == selected_industry)]\n",
    "\n",
    "    if not filtered_sales.empty:\n",
    "        break  # 유효한 데이터가 있으면 반복 종료\n",
    "    else:\n",
    "        print(\"⚠️ 입력한 상권명 또는 업종명이 데이터에 없습니다. 다시 입력하세요.\\n\")\n",
    "\n",
    "# 4. 해당 상권이 속한 자치구 찾기\n",
    "selected_district_code = filtered_sales[\"자치구_코드_명\"].iloc[0]\n",
    "\n",
    "# 5. 해당 자치구 전체 매출 데이터 가져오기\n",
    "district_sales = sales_df[sales_df[\"자치구_코드_명\"] == selected_district_code]\n",
    "\n",
    "# 6. 분기별 자치구 전체 매출 합산\n",
    "district_sales_grouped = district_sales.groupby(\"기준_년분기\")[\"당월_매출_금액\"].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={\"당월_매출_금액\": \"자치구_총매출\"}, inplace=True)\n",
    "\n",
    "# 7. 해당 상권이 차지하는 비율 계산\n",
    "sales_with_ratio = filtered_sales.merge(district_sales_grouped, on=\"기준_년분기\")\n",
    "sales_with_ratio[\"매출_비율\"] = sales_with_ratio[\"당월_매출_금액\"] / sales_with_ratio[\"자치구_총매출\"]\n",
    "\n",
    "# 8. 유동인구 데이터 연결 (자치구 기준)\n",
    "pop_df.rename(columns={\"시군구명\": \"자치구_코드_명\", \"총생활인구수\": \"유동인구\"}, inplace=True)\n",
    "sales_with_ratio = sales_with_ratio.merge(pop_df[[\"자치구_코드_명\", \"유동인구\"]], on=\"자치구_코드_명\", how=\"left\")\n",
    "\n",
    "# 9. 경제심리지수 및 소비자물가지수 연결\n",
    "econ_df[\"날짜\"] = pd.to_datetime(econ_df[\"날짜\"])\n",
    "cpi_df[\"날짜\"] = pd.to_datetime(cpi_df[\"날짜\"])\n",
    "\n",
    "econ_df[\"기준_년분기\"] = econ_df[\"날짜\"].dt.to_period(\"Q\").astype(str)\n",
    "cpi_df[\"기준_년분기\"] = cpi_df[\"날짜\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "sales_with_ratio = sales_with_ratio.merge(econ_df[[\"기준_년분기\", \"경제심리지수\"]], on=\"기준_년분기\", how=\"left\")\n",
    "sales_with_ratio = sales_with_ratio.merge(cpi_df[[\"기준_년분기\", \"소비자물가지수\"]], on=\"기준_년분기\", how=\"left\")\n",
    "\n",
    "# 10. 학습용 데이터 준비\n",
    "features = [\"유동인구\", \"경제심리지수\", \"소비자물가지수\"]\n",
    "target = \"매출_비율\"\n",
    "\n",
    "sales_with_ratio.dropna(inplace=True)  # 결측치 제거\n",
    "X = sales_with_ratio[features]\n",
    "y = sales_with_ratio[target]\n",
    "\n",
    "# 11. 학습/검증 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 12. 모델 학습\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. 검증 데이터 평가\n",
    "y_pred = model.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n📊 모델 검증 결과\")\n",
    "print(f\"MAPE (평균 절대 퍼센트 오차): {mape:.4f}\")\n",
    "print(f\"RMSE (평균 제곱근 오차): {rmse:.4f}\\n\")\n",
    "\n",
    "# 14. 전체 데이터로 다시 학습 후 2024년 3분기 예측\n",
    "model.fit(X, y)\n",
    "\n",
    "# 15. 2024년 3분기 예측을 위한 데이터 생성\n",
    "future_quarter = \"2024Q3\"\n",
    "latest_cpi = cpi_df[cpi_df[\"기준_년분기\"] == future_quarter][\"소비자물가지수\"].values[0]\n",
    "latest_econ = econ_df[econ_df[\"기준_년분기\"] == future_quarter][\"경제심리지수\"].values[0]\n",
    "latest_pop = pop_df[pop_df[\"자치구_코드_명\"] == selected_district_code][\"유동인구\"].mean()\n",
    "\n",
    "future_X = pd.DataFrame([[latest_pop, latest_econ, latest_cpi]], columns=features)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 16. 2024년 3분기 자치구 매출 예측 후 상권별 적용\n",
    "district_total_future_sales = district_sales_grouped[district_sales_grouped[\"기준_년분기\"] == future_quarter][\"자치구_총매출\"].values[0]\n",
    "predicted_sales = district_total_future_sales * predicted_ratio\n",
    "\n",
    "print(f\"📌 선택한 상권: {selected_district}\")\n",
    "print(f\"📌 선택한 업종: {selected_industry}\")\n",
    "print(f\"📈 예측된 2024년 3분기 매출: {predicted_sales:,.0f} 원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1169470658.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 39\u001b[1;36m\u001b[0m\n\u001b[1;33m    sales_ratio = filtered_sales.groupby(\"기준_년분기\")[\"당월_매출_금액\"].sum() /\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 파일 경로 설정\n",
    "sales_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\구 데이터\\\\피처엔지니어링한통합데이터\\\\피처엔지니어링일단다한통합데이터.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\서울 일별 유동인구.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\전처리 데이터\\\\경제심리지수.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\전처리 데이터\\\\소비자물가지수.csv\"\n",
    "\n",
    "# 데이터 로드\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 사용자 입력\n",
    "selected_district = input(\"자치구를 입력하세요: \")\n",
    "selected_business = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 2019~2024년 2분기까지의 데이터 필터링\n",
    "sales_df = sales_df[sales_df['기준_년분기_코드'] <= 20242]\n",
    "filtered_sales = sales_df[(sales_df['상권_코드_명'] == selected_district) & \n",
    "                           (sales_df['서비스_업종_코드_명'] == selected_business)]\n",
    "\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"선택한 상권-업종 데이터가 없습니다.\")\n",
    "\n",
    "# 자치구 매출 총합 계산\n",
    "sales_df['기준_년분기'] = sales_df['기준_년분기_코드']\n",
    "district_sales_grouped = sales_df.groupby(['기준_년분기', '자치구_코드_명'])['당월_매출_금액'].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={'당월_매출_금액': '자치구_총매출'}, inplace=True)\n",
    "\n",
    "# 선택한 상권의 자치구 찾기\n",
    "selected_district_code = filtered_sales[\"자치구_코드_명\"].iloc[0]\n",
    "district_sales_filtered = district_sales_grouped[district_sales_grouped['자치구_코드_명'] == selected_district_code]\n",
    "\n",
    "# 매출 비율 계산\n",
    "sales_ratio = filtered_sales.groupby(\"기준_년분기\")[\"당월_매출_금액\"].sum() / \n",
    "district_sales_filtered.groupby(\"기준_년분기\")[\"자치구_총매출\"].sum()\n",
    "sales_ratio = sales_ratio.fillna(0).reset_index()\n",
    "sales_ratio.rename(columns={\"당월_매출_금액\": \"매출_비율\"}, inplace=True)\n",
    "\n",
    "# 유동인구, 경제심리지수, 소비자물가지수 결합\n",
    "econ_df['년월'] = econ_df['날짜'].str[:7]\n",
    "cpi_df['년월'] = cpi_df['날짜'].str[:7]\n",
    "pop_df['년월'] = pop_df['시군구명'].map(lambda x: str(x)[:7])\n",
    "merged_data = sales_ratio.merge(pop_df, left_on='기준_년분기', right_on='년월', how='left')\n",
    "merged_data = merged_data.merge(econ_df, on='년월', how='left')\n",
    "merged_data = merged_data.merge(cpi_df, on='년월', how='left')\n",
    "\n",
    "# 학습 데이터 준비\n",
    "X = merged_data[['총생활인구수', '경제심리지수', '소비자물가지수']]\n",
    "y = merged_data['매출_비율']\n",
    "\n",
    "# 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 미래 데이터 예측\n",
    "future_data = merged_data.iloc[-1].copy()\n",
    "future_data['기준_년분기'] = 20243\n",
    "future_X = future_data[['총생활인구수', '경제심리지수', '소비자물가지수']].values.reshape(1, -1)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 2024년 3분기 자치구 매출 예측 후 상권별 적용\n",
    "future_quarter = 20243\n",
    "future_district_sales = district_sales_filtered.iloc[-1]['자치구_총매출'] * 1.02  # 성장률 적용 (가정치)\n",
    "predicted_sales = future_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"📌 선택한 상권: {selected_district}\")\n",
    "print(f\"📌 선택한 업종: {selected_business}\")\n",
    "print(f\"📊 예측된 2024년 3분기 매출: {predicted_sales:,.0f} 원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "선택한 자치구와 업종 데이터가 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m filtered_sales \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구_코드_명\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m서비스_업종_코드_명\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_sector)]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_sales\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m선택한 자치구와 업종 데이터가 없습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 4. 자치구 전체 매출 데이터 계산\u001b[39;00m\n\u001b[0;32m     28\u001b[0m district_sales_grouped \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구_코드_명\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m기준_년분기_코드\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m당월_매출_금액\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mValueError\u001b[0m: 선택한 자치구와 업종 데이터가 없습니다."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\구 데이터\\\\피처엔지니어링한통합데이터\\\\피처엔지니어링일단다한통합데이터.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\서울 일별 유동인구.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\전처리 데이터\\\\경제심리지수.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\머신러닝 사용 데이터\\\\전처리 데이터\\\\소비자물가지수.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. 사용자 입력 받기\n",
    "selected_district = input(\"자치구명을 입력하세요: \")\n",
    "selected_sector = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 3. 선택한 상권과 업종 데이터 필터링\n",
    "filtered_sales = df[(df['자치구_코드_명'] == selected_district) & (df['서비스_업종_코드_명'] == selected_sector)]\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"선택한 자치구와 업종 데이터가 없습니다.\")\n",
    "\n",
    "# 4. 자치구 전체 매출 데이터 계산\n",
    "district_sales_grouped = df[df['자치구_코드_명'] == selected_district].groupby('기준_년분기_코드')['당월_매출_금액'].sum().reset_index()\n",
    "sector_sales_grouped = filtered_sales.groupby('기준_년분기_코드')['당월_매출_금액'].sum().reset_index()\n",
    "\n",
    "# 5. 매출 비율 계산 및 학습 데이터 준비\n",
    "merged_sales = pd.merge(sector_sales_grouped, district_sales_grouped, on='기준_년분기_코드', suffixes=('_업종', '_자치구'))\n",
    "merged_sales['매출비율'] = merged_sales['당월_매출_금액_업종'] / merged_sales['당월_매출_금액_자치구']\n",
    "\n",
    "# 6. 외부 데이터 결합 (경제심리지수, 소비자물가지수)\n",
    "econ_df['날짜'] = econ_df['날짜'].astype(str).str[:6].astype(int)\n",
    "cpi_df['날짜'] = cpi_df['날짜'].astype(str).str[:6].astype(int)\n",
    "\n",
    "merged_sales = merged_sales.merge(econ_df, left_on='기준_년분기_코드', right_on='날짜', how='left')\n",
    "merged_sales = merged_sales.merge(cpi_df, left_on='기준_년분기_코드', right_on='날짜', how='left')\n",
    "merged_sales.drop(columns=['날짜_x', '날짜_y'], inplace=True)\n",
    "\n",
    "# 7. 학습 데이터 및 검증 데이터 분할\n",
    "X = merged_sales[['경제심리지수', '소비자물가지수']]\n",
    "y = merged_sales['매출비율']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9. 모델 검증\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"검증 결과: MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "# 10. 2024년 3분기 예측 데이터 준비\n",
    "future_quarter = 20243\n",
    "future_X = X.iloc[[-1]]  # 가장 최근 데이터를 기반으로 예측\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 11. 2024년 3분기 자치구 예상 매출 계산\n",
    "latest_district_sales = district_sales_grouped[district_sales_grouped['기준_년분기_코드'] == 20242]['당월_매출_금액'].values[0]\n",
    "predicted_district_sales = latest_district_sales * 1.02  # 자치구 매출이 2% 성장한다고 가정\n",
    "predicted_sales = predicted_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"📌 선택한 자치구: {selected_district}, 선택한 업종: {selected_sector}\")\n",
    "print(f\"✅ 2024년 3분기 예측 매출: {predicted_sales:,.0f}원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'시군구명'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15336\\570041857.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'시군구명'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'자치구_코드_명'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'자치구_코드_명'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'총생활인구수'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# 🔥 수정된 병합 코드: 컬럼명이 다름을 고려하여 `left_on`, `right_on` 사용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m district_sales_grouped = district_sales_grouped.merge(\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mpopulation_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'자치구_코드_명'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'시군구명'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '시군구명'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 파일 로드\n",
    "sales_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\구 데이터\\피처엔지니어링한통합데이터\\피처엔지니어링일단다한통합데이터.csv\")\n",
    "population_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\서울 일별 유동인구.csv\")\n",
    "economic_index_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\경제심리지수.csv\")\n",
    "cpi_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\소비자물가지수.csv\")\n",
    "\n",
    "# 사용자 입력값 받기\n",
    "selected_district = input(\"상권명을 입력하세요: \")\n",
    "selected_industry = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 1. 선택된 상권-업종의 매출 데이터 필터링\n",
    "filtered_sales = sales_df[\n",
    "    (sales_df['상권_코드_명'] == selected_district) & \n",
    "    (sales_df['서비스_업종_코드_명'] == selected_industry)\n",
    "]\n",
    "\n",
    "# 2. 선택된 상권이 포함된 자치구 구하기\n",
    "selected_districts = filtered_sales['자치구_코드_명'].unique()\n",
    "\n",
    "# 3. 해당 자치구와 업종의 매출 총합 구하기\n",
    "district_sales_grouped = sales_df[\n",
    "    (sales_df['자치구_코드_명'].isin(selected_districts)) & \n",
    "    (sales_df['서비스_업종_코드_명'] == selected_industry)\n",
    "].groupby('기준_년분기_코드')['당월_매출_금액'].sum().reset_index()\n",
    "\n",
    "# 4. 선택된 상권-업종의 매출 비율 계산\n",
    "selected_total_sales = filtered_sales.groupby('기준_년분기_코드')['당월_매출_금액'].sum().reset_index()\n",
    "merged_sales = selected_total_sales.merge(district_sales_grouped, on='기준_년분기_코드', suffixes=('_상권', '_자치구'))\n",
    "merged_sales['매출_비율'] = merged_sales['당월_매출_금액_상권'] / merged_sales['당월_매출_금액_자치구']\n",
    "\n",
    "# 5. 유동인구 데이터 전처리 및 병합\n",
    "population_df = population_df.rename(columns={'시군구명': '자치구_코드_명'})\n",
    "population_df = population_df.groupby('자치구_코드_명')['총생활인구수'].mean().reset_index()\n",
    "\n",
    "# 🔥 수정된 병합 코드: 컬럼명이 다름을 고려하여 `left_on`, `right_on` 사용\n",
    "district_sales_grouped = district_sales_grouped.merge(\n",
    "    population_df, left_on='자치구_코드_명', right_on='시군구명', how='left'\n",
    ")\n",
    "\n",
    "# 6. 경제심리지수 및 소비자물가지수 병합\n",
    "economic_index_df['날짜'] = pd.to_datetime(economic_index_df['날짜'])\n",
    "cpi_df['날짜'] = pd.to_datetime(cpi_df['날짜'])\n",
    "\n",
    "# 7. 2019~2024년 2분기까지의 데이터 필터링\n",
    "district_sales_grouped['기준_년분기_코드'] = district_sales_grouped['기준_년분기_코드'].astype(str)\n",
    "filtered_data = district_sales_grouped[district_sales_grouped['기준_년분기_코드'].str[:4].astype(int) < 20243]\n",
    "\n",
    "# 8. 머신러닝 모델 학습 준비\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "features = filtered_data[['기준_년분기_코드', '총생활인구수']]\n",
    "target = filtered_data['당월_매출_금액']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9. 모델 학습 및 예측\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 10. 모델 평가 및 미래 매출 예측\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"모델 MAE(평균 절대 오차): {mae}\")\n",
    "\n",
    "# 11. 2024년 3분기 매출 예측\n",
    "future_quarter = pd.DataFrame({'기준_년분기_코드': [20243], '총생활인구수': [population_df['총생활인구수'].mean()]})\n",
    "predicted_sales = model.predict(future_quarter)\n",
    "\n",
    "# 12. 최종 예측 결과 출력\n",
    "predicted_sales_value = predicted_sales[0] * merged_sales['매출_비율'].mean()\n",
    "print(f\"예측된 2024년 3분기 {selected_district} {selected_industry} 매출: {predicted_sales_value:.2f} 원\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신규 데이터 생성(매출 데이터 추출, 유동인구 데이터와 병합합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 병합 완료! 저장된 파일: C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\병합된_매출_유동인구.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📌 1. 매출 데이터 불러오기\n",
    "sales_file = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\구 데이터\\피처엔지니어링한통합데이터\\피처엔지니어링일단다한통합데이터.csv\"\n",
    "sales_df = pd.read_csv(sales_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "sales_df = sales_df[['기준_년분기_코드', '상권_코드_명', '서비스_업종_코드_명', '당월_매출_금액', '당월_매출_건수', '총_유동인구_수', '자치구_코드_명']]\n",
    "\n",
    "# 📌 2. 유동인구 데이터 불러오기\n",
    "pop_file = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\서울 일별 유동인구.csv\"\n",
    "pop_df = pd.read_csv(pop_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 기준일ID를 연분기 형식으로 변환 (예: 20180416 -> 20181)\n",
    "pop_df['기준_년분기_코드'] = (pop_df['기준일ID'] // 10000) * 10 + ((pop_df['기준일ID'] % 10000) // 3000 + 1)\n",
    "\n",
    "# 일별 데이터를 분기별로 집계\n",
    "pop_quarterly_df = pop_df.groupby(['기준_년분기_코드', '시군구명'], as_index=False)[\n",
    "    ['총생활인구수', '일최대인구수', '일최소인구수', '일최대이동인구수', '서울외유입인구수']\n",
    "].sum()\n",
    "\n",
    "# 컬럼명 변경 (매출 데이터와 일관성 유지)\n",
    "pop_quarterly_df = pop_quarterly_df.rename(columns={'시군구명': '자치구_코드_명'})\n",
    "\n",
    "# 📌 3. 데이터 병합\n",
    "merged_df = sales_df.merge(pop_quarterly_df, on=['기준_년분기_코드', '자치구_코드_명'], how='left')\n",
    "\n",
    "# 📌 4. CSV 파일로 저장\n",
    "output_file = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\병합된_매출_유동인구.csv\"\n",
    "merged_df.to_csv(output_file, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "print(f\"데이터 병합 완료! 저장된 파일: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 결과 - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024년 3분기의 예상 매출: 101649046.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 데이터 로드\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\병합된_매출_유동인구.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\소비자물가지수.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\경제심리지수.csv\")\n",
    "\n",
    "# 날짜 형식 변환\n",
    "cpi_data['날짜'] = pd.to_datetime(cpi_data['날짜'])\n",
    "esi_data['날짜'] = pd.to_datetime(esi_data['날짜'])\n",
    "\n",
    "# 분기 코드로 변환\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + (date.month - 1) // 3 + 1\n",
    "\n",
    "cpi_data['기준_년분기_코드'] = cpi_data['날짜'].apply(get_quarter_code)\n",
    "esi_data['기준_년분기_코드'] = esi_data['날짜'].apply(get_quarter_code)\n",
    "\n",
    "# 분기별 평균 계산\n",
    "cpi_quarterly = cpi_data.groupby('기준_년분기_코드')['소비자물가지수'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('기준_년분기_코드')['경제심리지수'].mean().reset_index()\n",
    "\n",
    "# 경제심리지수는 직전 분기의 평균값 사용\n",
    "esi_quarterly['경제심리지수'] = esi_quarterly['경제심리지수'].shift(1)\n",
    "\n",
    "# 사용자 입력값 받기\n",
    "region_name = input(\"상권명을 입력하세요: \")\n",
    "industry_name = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 상권 및 업종에 해당하는 데이터 필터링\n",
    "filtered_sales = sales_data[(sales_data['상권_코드_명'] == region_name) & \n",
    "                            (sales_data['서비스_업종_코드_명'] == industry_name)]\n",
    "\n",
    "# 자치구별 매출 총합 계산\n",
    "region_sales = sales_data.groupby(['기준_년분기_코드', '자치구_코드_명'])['당월_매출_금액'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['기준_년분기_코드', '자치구_코드_명'], suffixes=('', '_자치구총합'))\n",
    "\n",
    "# 매출 비율 계산\n",
    "filtered_sales['매출비율'] = filtered_sales['당월_매출_금액'] / filtered_sales['당월_매출_금액_자치구총합']\n",
    "\n",
    "# 소비자물가지수와 경제심리지수를 병합\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='기준_년분기_코드', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='기준_년분기_코드', how='left')\n",
    "\n",
    "# 학습 및 타겟 변수 설정\n",
    "X = filtered_sales[['소비자물가지수', '경제심리지수', '총생활인구수', '일최대인구수', '일최소인구수', '일최대이동인구수', '서울외유입인구수']]\n",
    "y = filtered_sales['당월_매출_금액']\n",
    "\n",
    "# 학습-검증 데이터 분리 (8:2 비율)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 결과 출력\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"검증 결과 - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024년 3분기의 소비자물가지수와 경제심리지수를 가져옴\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['기준_년분기_코드'] == 20243]['소비자물가지수'].values[0]\n",
    "q3_previous_qtr_code = 20242  # 직전 분기 코드\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['기준_년분기_코드'] == q3_previous_qtr_code]['경제심리지수'].values[0]\n",
    "\n",
    "# 예측을 위한 입력값 생성 (임의로 인구 데이터를 평균값으로 설정)\n",
    "input_features = [[q3_cpi, q3_previous_qtr_avg_esi, \n",
    "                   filtered_sales['총생활인구수'].mean(), \n",
    "                   filtered_sales['일최대인구수'].mean(), \n",
    "                   filtered_sales['일최소인구수'].mean(), \n",
    "                   filtered_sales['일최대이동인구수'].mean(), \n",
    "                   filtered_sales['서울외유입인구수'].mean()]]\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024년 3분기의 예상 매출: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 결과 - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024년 3분기의 예상 매출: 101649046.18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 데이터 로드\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\병합된_매출_유동인구.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\소비자물가지수.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\경제심리지수.csv\")\n",
    "\n",
    "# 날짜 형식 변환\n",
    "cpi_data['날짜'] = pd.to_datetime(cpi_data['날짜'])\n",
    "esi_data['날짜'] = pd.to_datetime(esi_data['날짜'])\n",
    "\n",
    "# 분기 코드로 변환\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['기준_년분기_코드'] = cpi_data['날짜'].apply(get_quarter_code)\n",
    "esi_data['기준_년분기_코드'] = esi_data['날짜'].apply(get_quarter_code)\n",
    "\n",
    "# 분기별 평균 계산\n",
    "cpi_quarterly = cpi_data.groupby('기준_년분기_코드')['소비자물가지수'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('기준_년분기_코드')['경제심리지수'].mean().reset_index()\n",
    "\n",
    "# 경제심리지수는 직전 분기의 평균값 사용\n",
    "esi_quarterly['경제심리지수'] = esi_quarterly['경제심리지수'].shift(1)\n",
    "\n",
    "# 사용자 입력값 받기\n",
    "region_name = input(\"상권명을 입력하세요: \")\n",
    "industry_name = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 상권 및 업종에 해당하는 데이터 필터링\n",
    "filtered_sales = sales_data[(sales_data['상권_코드_명'] == region_name) & \n",
    "                            (sales_data['서비스_업종_코드_명'] == industry_name)]\n",
    "\n",
    "# 자치구별 매출 총합 계산\n",
    "region_sales = sales_data.groupby(['기준_년분기_코드', '자치구_코드_명'])['당월_매출_금액'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['기준_년분기_코드', '자치구_코드_명'], \n",
    "                                      suffixes=('', '_자치구총합'))\n",
    "\n",
    "# 매출 비율 계산\n",
    "filtered_sales['매출비율'] = filtered_sales['당월_매출_금액'] / filtered_sales['당월_매출_금액_자치구총합']\n",
    "\n",
    "# 소비자물가지수와 경제심리지수를 병합\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='기준_년분기_코드', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='기준_년분기_코드', how='left')\n",
    "\n",
    "# 학습 및 타겟 변수 설정\n",
    "feature_columns = ['소비자물가지수', '경제심리지수', '총생활인구수', '일최대인구수', \n",
    "                   '일최소인구수', '일최대이동인구수', '서울외유입인구수']\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['당월_매출_금액']\n",
    "\n",
    "# 학습-검증 데이터 분리 (8:2 비율)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 결과 출력\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"검증 결과 - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024년 3분기의 소비자물가지수와 경제심리지수를 가져옴\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['기준_년분기_코드'] == 20243]['소비자물가지수'].values[0]\n",
    "q3_previous_qtr_code = 20242  # 직전 분기 코드\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['기준_년분기_코드'] == q3_previous_qtr_code]['경제심리지수'].values[0]\n",
    "\n",
    "# 예측을 위한 입력값 생성 (임의로 인구 데이터를 평균값으로 설정)\n",
    "input_data = {\n",
    "    '소비자물가지수': [q3_cpi],\n",
    "    '경제심리지수': [q3_previous_qtr_avg_esi],\n",
    "    '총생활인구수': [filtered_sales['총생활인구수'].mean()],\n",
    "    '일최대인구수': [filtered_sales['일최대인구수'].mean()],\n",
    "    '일최소인구수': [filtered_sales['일최소인구수'].mean()],\n",
    "    '일최대이동인구수': [filtered_sales['일최대이동인구수'].mean()],\n",
    "    '서울외유입인구수': [filtered_sales['서울외유입인구수'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrame으로 생성하여 학습 시 사용한 feature 이름을 동일하게 지정\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024년 3분기의 예상 매출: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 모델(XGboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.6/124.9 MB 39.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 13.1/124.9 MB 32.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 19.7/124.9 MB 33.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 24.9/124.9 MB 30.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 30.4/124.9 MB 30.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 29.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 39.3/124.9 MB 27.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.8/124.9 MB 27.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 49.8/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 59.8/124.9 MB 26.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.0/124.9 MB 26.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 68.7/124.9 MB 25.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 71.8/124.9 MB 25.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.5/124.9 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.4/124.9 MB 24.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 82.1/124.9 MB 23.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 84.9/124.9 MB 23.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 87.8/124.9 MB 22.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 92.3/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 97.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 102.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.5/124.9 MB 23.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.7/124.9 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.0/124.9 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 23.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 22.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 결과 - MAE: 12164253.54, RMSE: 14829197.65\n",
      "2024년 3분기의 예상 매출: 105060776.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 데이터 로드\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\병합된_매출_유동인구.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\소비자물가지수.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\전처리 데이터\\경제심리지수.csv\")\n",
    "\n",
    "# 날짜 형식 변환\n",
    "cpi_data['날짜'] = pd.to_datetime(cpi_data['날짜'])\n",
    "esi_data['날짜'] = pd.to_datetime(esi_data['날짜'])\n",
    "\n",
    "# 분기 코드로 변환\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['기준_년분기_코드'] = cpi_data['날짜'].apply(get_quarter_code)\n",
    "esi_data['기준_년분기_코드'] = esi_data['날짜'].apply(get_quarter_code)\n",
    "\n",
    "# 분기별 평균 계산\n",
    "cpi_quarterly = cpi_data.groupby('기준_년분기_코드')['소비자물가지수'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('기준_년분기_코드')['경제심리지수'].mean().reset_index()\n",
    "\n",
    "# 경제심리지수는 직전 분기의 평균값 사용\n",
    "esi_quarterly['경제심리지수'] = esi_quarterly['경제심리지수'].shift(1)\n",
    "\n",
    "# 사용자 입력값 받기\n",
    "region_name = input(\"상권명을 입력하세요: \")\n",
    "industry_name = input(\"업종명을 입력하세요: \")\n",
    "\n",
    "# 상권 및 업종에 해당하는 데이터 필터링\n",
    "filtered_sales = sales_data[\n",
    "    (sales_data['상권_코드_명'] == region_name) & \n",
    "    (sales_data['서비스_업종_코드_명'] == industry_name)\n",
    "]\n",
    "\n",
    "# 자치구별 매출 총합 계산\n",
    "region_sales = sales_data.groupby(['기준_년분기_코드', '자치구_코드_명'])['당월_매출_금액'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(\n",
    "    region_sales, \n",
    "    on=['기준_년분기_코드', '자치구_코드_명'], \n",
    "    suffixes=('', '_자치구총합')\n",
    ")\n",
    "\n",
    "# 매출 비율 계산\n",
    "filtered_sales['매출비율'] = filtered_sales['당월_매출_금액'] / filtered_sales['당월_매출_금액_자치구총합']\n",
    "\n",
    "# 소비자물가지수와 경제심리지수를 병합\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='기준_년분기_코드', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='기준_년분기_코드', how='left')\n",
    "\n",
    "# 학습 및 타겟 변수 설정\n",
    "feature_columns = [\n",
    "    '소비자물가지수', '경제심리지수', '총생활인구수', \n",
    "    '일최대인구수', '일최소인구수', '일최대이동인구수', '서울외유입인구수'\n",
    "]\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['당월_매출_금액']\n",
    "\n",
    "# 학습-검증 데이터 분리 (8:2 비율)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost 모델 적용 (랜덤포레스트보다 일반적으로 좋은 성능을 기대할 수 있음)\n",
    "model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 결과 출력\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"검증 결과 - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024년 3분기의 소비자물가지수와 경제심리지수를 가져옴\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['기준_년분기_코드'] == 20243]['소비자물가지수'].values[0]\n",
    "q3_previous_qtr_code = 20242  # 직전 분기 코드\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[\n",
    "    esi_quarterly['기준_년분기_코드'] == q3_previous_qtr_code\n",
    "]['경제심리지수'].values[0]\n",
    "\n",
    "# 예측을 위한 입력값 생성 (임의로 인구 데이터를 평균값으로 설정)\n",
    "input_data = {\n",
    "    '소비자물가지수': [q3_cpi],\n",
    "    '경제심리지수': [q3_previous_qtr_avg_esi],\n",
    "    '총생활인구수': [filtered_sales['총생활인구수'].mean()],\n",
    "    '일최대인구수': [filtered_sales['일최대인구수'].mean()],\n",
    "    '일최소인구수': [filtered_sales['일최소인구수'].mean()],\n",
    "    '일최대이동인구수': [filtered_sales['일최대이동인구수'].mean()],\n",
    "    '서울외유입인구수': [filtered_sales['서울외유입인구수'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrame으로 생성하여 학습 시 사용한 feature 이름을 동일하게 지정\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024년 3분기의 예상 매출: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 데이터 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralprophet\n",
      "  Downloading neuralprophet-0.8.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting captum>=0.6.0 (from neuralprophet)\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: holidays>=0.41 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.66)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.5.3 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (3.9.2)\n",
      "Requirement already satisfied: nbformat<6.0.0,>=5.8.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.10.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.2.2)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.24.1)\n",
      "Collecting pytorch-lightning<2.0.0,>=1.9.4 (from neuralprophet)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: tensorboard<3.0.0,>=2.11.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.18.0)\n",
      "Collecting torch<3.0.0,>=2.0.0 (from neuralprophet)\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchmetrics<2.0.0,>=1.0.0 (from neuralprophet)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (4.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\m\\anaconda3\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\m\\anaconda3\\lib\\site-packages (from holidays>=0.41->neuralprophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (3.1.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from plotly<6.0.0,>=5.13.1->neuralprophet) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2024.6.1)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet)\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch<3.0.0,>=2.0.0->neuralprophet)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->neuralprophet) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\anaconda3\\lib\\site-packages (from tqdm->captum>=0.6.0->neuralprophet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.7)\n",
      "Downloading neuralprophet-0.8.0-py3-none-any.whl (145 kB)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "   ---------------------------------------- 0.0/829.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 829.5/829.5 kB 35.5 MB/s eta 0:00:00\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 7.9/204.1 MB 40.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 18.6/204.1 MB 46.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 29.4/204.1 MB 49.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 40.1/204.1 MB 50.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 49.8/204.1 MB 49.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 59.5/204.1 MB 49.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 67.9/204.1 MB 48.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 77.6/204.1 MB 48.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 85.5/204.1 MB 47.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 93.8/204.1 MB 46.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 101.4/204.1 MB 45.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 107.5/204.1 MB 44.3 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.1/204.1 MB 44.1 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 124.3/204.1 MB 43.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 132.6/204.1 MB 43.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 140.5/204.1 MB 43.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 148.9/204.1 MB 43.0 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 158.1/204.1 MB 43.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 161.0/204.1 MB 41.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 168.0/204.1 MB 41.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 175.4/204.1 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 182.2/204.1 MB 40.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.7/204.1 MB 40.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 192.2/204.1 MB 39.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.8/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.6/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "   ---------------------------------------- 0.0/927.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 927.3/927.3 kB 21.5 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: sympy, lightning-utilities, torch, torchmetrics, captum, pytorch-lightning, neuralprophet\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed captum-0.7.0 lightning-utilities-0.12.0 neuralprophet-0.8.0 pytorch-lightning-1.9.5 sympy-1.13.1 torch-2.6.0 torchmetrics-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neuralprophet in c:\\users\\m\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: captum>=0.6.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.7.0)\n",
      "Requirement already satisfied: holidays>=0.41 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.66)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.5.3 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (3.9.2)\n",
      "Requirement already satisfied: nbformat<6.0.0,>=5.8.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.10.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.2.2)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.24.1)\n",
      "Requirement already satisfied: pytorch-lightning<2.0.0,>=1.9.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.9.5)\n",
      "Requirement already satisfied: tensorboard<3.0.0,>=2.11.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.18.0)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.6.0)\n",
      "Requirement already satisfied: torchmetrics<2.0.0,>=1.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.6.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (4.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\m\\anaconda3\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\m\\anaconda3\\lib\\site-packages (from holidays>=0.41->neuralprophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (3.1.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from plotly<6.0.0,>=5.13.1->neuralprophet) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (0.12.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->neuralprophet) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\anaconda3\\lib\\site-packages (from tqdm->captum>=0.6.0->neuralprophet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.863% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "WARNING - (NP.config.set_lr_finder_args) - Learning rate finder: The number of batches (46) is too small than the required number                     for the learning rate finder (229). The results might not be optimal.\n",
      "Missing logger folder: c:\\Users\\m\\Documents\\GitHub\\sda.final.project\\lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6fa684e2ea4df483fe2ca3d71b9156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 적용\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 적용\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralProphet()\n\u001b[1;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 결측치 예측\u001b[39;00m\n\u001b[0;32m     31\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1062\u001b[0m, in \u001b[0;36mNeuralProphet.fit\u001b[1;34m(self, df, freq, validation_df, epochs, batch_size, learning_rate, early_stopping, minimal, metrics, progress, checkpointing, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   1063\u001b[0m         df,\n\u001b[0;32m   1064\u001b[0m         progress_bar_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(progress),\n\u001b[0;32m   1065\u001b[0m         metrics_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics),\n\u001b[0;32m   1066\u001b[0m         checkpointing_enabled\u001b[38;5;241m=\u001b[39mcheckpointing,\n\u001b[0;32m   1067\u001b[0m         continue_training\u001b[38;5;241m=\u001b[39mcontinue_training,\n\u001b[0;32m   1068\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m     df_val, _, _, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(validation_df)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:2802\u001b[0m, in \u001b[0;36mNeuralProphet._train\u001b[1;34m(self, df, df_val, progress_bar_enabled, metrics_enabled, checkpointing_enabled, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   2800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mset_lr_finder_args(dataset_size\u001b[38;5;241m=\u001b[39mdataset_size, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;66;03m# Find suitable learning rate\u001b[39;00m\n\u001b[1;32m-> 2802\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_find(\n\u001b[0;32m   2803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   2804\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mlr_finder_args,\n\u001b[0;32m   2806\u001b[0m )\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lr_finder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;66;03m# Estimate the optimat learning rate from the loss curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:267\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    264\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mauto_lr_find \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1097\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m-> 1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1394\u001b[0m             fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:122\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_find(trainer, pl_module)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:107\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m lr_find(\n\u001b[0;32m    108\u001b[0m             trainer,\n\u001b[0;32m    109\u001b[0m             pl_module,\n\u001b[0;32m    110\u001b[0m             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_lr,\n\u001b[0;32m    111\u001b[0m             max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_lr,\n\u001b[0;32m    112\u001b[0m             num_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_training_steps,\n\u001b[0;32m    113\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode,\n\u001b[0;32m    114\u001b[0m             early_stop_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_stop_threshold,\n\u001b[0;32m    115\u001b[0m             update_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attr,\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:273\u001b[0m, in \u001b[0;36mlr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    270\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Restore initial state of model\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore(ckpt_path)\n\u001b[0;32m    274\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mremove_checkpoint(ckpt_path)\n\u001b[0;32m    275\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# reset restarting flag as checkpoint restoring sets it to True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:224\u001b[0m, in \u001b[0;36mCheckpointConnector.restore\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    state-restore, in this priority:\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        checkpoint_path: Path to a PyTorch Lightning checkpoint file.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# restore module states\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:90\u001b[0m, in \u001b[0;36mCheckpointConnector.resume_start\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 90\u001b[0m     loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:359\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    358\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\plugins\\io\\torch_io.py:86\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[1;34m(self, path, map_location)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\")\n",
    "\n",
    "data['날짜'] = pd.to_datetime(data['날짜'])  # 날짜 변환\n",
    "\n",
    "# 0인 값을 NaN으로 변경\n",
    "data.loc[data['총_결제금액'] == 0, '총_결제금액'] = np.nan\n",
    "data.loc[data['총_결제건수'] == 0, '총_결제건수'] = np.nan\n",
    "\n",
    "# 자치구별로 처리\n",
    "def fill_missing_values(group, target):\n",
    "    df = group[['날짜', target]].copy()\n",
    "    df = df.rename(columns={'날짜': 'ds', target: 'y'})\n",
    "    \n",
    "    # 학습 데이터와 예측할 데이터 분리\n",
    "    train_df = df.dropna()\n",
    "    predict_df = df[df['y'].isna()]\n",
    "    \n",
    "    if train_df.empty or predict_df.empty:\n",
    "        return group  # 학습할 데이터가 없으면 원본 반환\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = NeuralProphet()\n",
    "    model.fit(train_df, freq='D')\n",
    "    \n",
    "    # 결측치 예측\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 예측값 채우기\n",
    "    group.loc[group[target].isna(), target] = forecast['yhat1'].values\n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv(\"filled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "Missing logger folder: c:\\Users\\m\\Documents\\GitHub\\sda.final.project\\lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eb6aa3eba240f4a69b644a621a644e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 결측치 예측\u001b[39;00m\n\u001b[0;32m     33\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 34\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 예측 결과 반영\u001b[39;00m\n\u001b[0;32m     37\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mmerge(forecast[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기 (예제, 실제 경로에 맞게 수정)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 날짜 컬럼을 datetime으로 변환\n",
    "data['날짜'] = pd.to_datetime(data['날짜'])\n",
    "\n",
    "# NeuralProphet을 이용한 결측치 보정 함수\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='날짜')\n",
    "    \n",
    "    # 학습 데이터 준비\n",
    "    train_df = group[['날짜', target]].dropna().rename(columns={'날짜': 'ds', target: 'y'})\n",
    "    predict_df = group[['날짜']].rename(columns={'날짜': 'ds'})\n",
    "    \n",
    "    if train_df.empty:\n",
    "        return group  # 결측치를 채울 수 없으므로 원본 반환\n",
    "\n",
    "    # NeuralProphet 모델 설정\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,  # 수동 학습률 설정\n",
    "        batch_size=16,        # 배치 크기 수동 설정\n",
    "        daily_seasonality=True  # 일별 계절성 활성화\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(train_df, freq='D', progress='none', checkpointing=False)\n",
    "\n",
    "    # 결측치 예측\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 예측 결과 반영\n",
    "    group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "    group[target] = group[target].fillna(group['yhat1'])\n",
    "    group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 대해 결측치 보정 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"처리된 데이터가 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총_결제금액 학습 데이터 샘플 (상위 5개):\n",
      "            ds          y\n",
      "15  2020-04-28  106318760\n",
      "39  2020-04-29  177187910\n",
      "64  2020-04-30  149330922\n",
      "89  2020-05-01  150419200\n",
      "119 2020-05-02  111068980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f17b9d2d241e58815c565c6f1102e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 결측치 예측\u001b[39;00m\n\u001b[0;32m     37\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 38\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 예측 결과 반영\u001b[39;00m\n\u001b[0;32m     41\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mmerge(forecast[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기 (예제, 실제 경로에 맞게 수정)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 날짜 컬럼을 datetime으로 변환\n",
    "data['날짜'] = pd.to_datetime(data['날짜'])\n",
    "\n",
    "# NeuralProphet을 이용한 결측치 보정 함수\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='날짜').copy()\n",
    "    \n",
    "    # 학습 데이터 준비\n",
    "    train_df = group[['날짜', target]].dropna().rename(columns={'날짜': 'ds', target: 'y'}).copy()\n",
    "    predict_df = group[['날짜']].rename(columns={'날짜': 'ds'}).copy()\n",
    "\n",
    "    # 데이터 확인\n",
    "    if train_df.empty:\n",
    "        print(f\"⚠ {target} 컬럼의 모든 값이 결측치입니다. 학습을 건너뜁니다.\")\n",
    "        return group  # 결측치를 채울 수 없으므로 원본 반환\n",
    "    \n",
    "    print(f\"✅ {target} 학습 데이터 샘플 (상위 5개):\\n{train_df.head()}\")\n",
    "\n",
    "    # NeuralProphet 모델 설정\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,  # 학습률 설정\n",
    "        batch_size=16,        # 배치 크기 수동 설정\n",
    "        daily_seasonality=True  # 일별 계절성 활성화\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(train_df, freq='D', progress='none', checkpointing=False)\n",
    "\n",
    "    # 결측치 예측\n",
    "    future = predict_df[['ds']].copy()\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # 예측 결과 반영\n",
    "    group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "    group[target] = group[target].fillna(group['yhat1'])\n",
    "    group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 대해 결측치 보정 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"처리된 데이터가 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총_결제금액 학습 데이터 샘플 (상위 5개):\n",
      "            ds          y\n",
      "15  2020-04-28  106318760\n",
      "39  2020-04-29  177187910\n",
      "64  2020-04-30  149330922\n",
      "89  2020-05-01  150419200\n",
      "119 2020-05-02  111068980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a7e7259e9f43f3bf07cf85f49c1154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 대해 결측치 보정 적용\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 예측 수행\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(predict_df)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 예측 결과 반영\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m forecast\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기 (경로 수정 필요)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 날짜 컬럼을 datetime으로 변환\n",
    "data['날짜'] = pd.to_datetime(data['날짜'])\n",
    "\n",
    "# NeuralProphet을 이용한 결측치 보정 함수\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='날짜').copy()\n",
    "    \n",
    "    # 학습 데이터 준비 (결측치 제거)\n",
    "    train_df = group[['날짜', target]].dropna().rename(columns={'날짜': 'ds', target: 'y'}).copy()\n",
    "    predict_df = group[['날짜']].rename(columns={'날짜': 'ds'}).copy()\n",
    "\n",
    "    if train_df.empty:\n",
    "        print(f\"⚠ {target} 컬럼의 모든 값이 결측치입니다. 학습을 건너뜁니다.\")\n",
    "        return group  # 결측치를 채울 수 없으므로 원본 반환\n",
    "    \n",
    "    print(f\"✅ {target} 학습 데이터 샘플 (상위 5개):\\n{train_df.head()}\")\n",
    "\n",
    "    # NeuralProphet 모델 설정\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,\n",
    "        batch_size=16,\n",
    "        daily_seasonality=True\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    metrics = model.fit(train_df, freq='D', progress='none', epochs=100)\n",
    "\n",
    "    # 예측 수행\n",
    "    forecast = model.predict(predict_df)\n",
    "\n",
    "    # 예측 결과 반영\n",
    "    if 'yhat1' in forecast.columns:\n",
    "        group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "        group[target] = group[target].fillna(group['yhat1'])\n",
    "        group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 대해 결측치 보정 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"처리된 데이터가 {output_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ds'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m머신러닝 사용 데이터\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m머신러닝용_시계열데이터_null 제거.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 'ds' 컬럼이 날짜 형식인지 확인\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_missing_values\u001b[39m(group, target):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    특정 자치구 그룹의 target 컬럼(총_결제금액 또는 총_결제건수)의 결측치를 보정하는 함수\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# 데이터 로드 (예제)\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\")\n",
    "\n",
    "# 'ds' 컬럼이 날짜 형식인지 확인\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "def fill_missing_values(group, target):\n",
    "    \"\"\"\n",
    "    특정 자치구 그룹의 target 컬럼(총_결제금액 또는 총_결제건수)의 결측치를 보정하는 함수\n",
    "    \"\"\"\n",
    "    print(f\"처리 중인 그룹: {group['자치구'].iloc[0]}\")  # 그룹 확인\n",
    "    print(\"현재 컬럼 목록:\", group.columns)  # 컬럼 확인\n",
    "\n",
    "    if target not in group.columns:\n",
    "        print(f\"⚠️ '{target}' 컬럼이 존재하지 않음!\")\n",
    "        return group\n",
    "    \n",
    "    # 'ds'와 'target' 컬럼명을 NeuralProphet 형식에 맞게 변환\n",
    "    group = group.rename(columns={target: 'y'})\n",
    "\n",
    "    # 결측값을 포함한 데이터\n",
    "    train_df = group[['ds', 'y']].dropna()\n",
    "    predict_df = group[['ds', 'y']].copy()\n",
    "\n",
    "    print(\"📌 train_df 샘플:\", train_df.head())\n",
    "    print(\"📌 predict_df 샘플:\", predict_df.head())\n",
    "\n",
    "    if train_df.empty:\n",
    "        print(\"⚠️ 학습 데이터가 없음. 그룹을 그대로 반환합니다.\")\n",
    "        return group\n",
    "\n",
    "    # 모델 학습\n",
    "    model = NeuralProphet()\n",
    "    metrics = model.fit(train_df, freq='D', progress='none', epochs=100)\n",
    "\n",
    "    # 예측 수행\n",
    "    forecast = model.predict(predict_df)\n",
    "\n",
    "    # 예측 결과 반영\n",
    "    if 'yhat1' in forecast.columns:\n",
    "        group.loc[group['y'].isnull(), 'y'] = forecast['yhat1']\n",
    "\n",
    "    # 원래 컬럼명으로 되돌리기\n",
    "    group = group.rename(columns={'y': target})\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 대해 결측치 보정 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv(\"processed_data.csv\", index=False)\n",
    "print(\"✅ 결측치 보정 완료 및 저장\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.863% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "WARNING - (NP.config.set_lr_finder_args) - Learning rate finder: The number of batches (46) is too small than the required number                     for the learning rate finder (229). The results might not be optimal.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b50228b3de443019296ca3649716cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 적용\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 모든 자치구에 적용\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제금액\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m자치구\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총_결제건수\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralProphet(trainer_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[1;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 결측치 예측\u001b[39;00m\n\u001b[0;32m     31\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1062\u001b[0m, in \u001b[0;36mNeuralProphet.fit\u001b[1;34m(self, df, freq, validation_df, epochs, batch_size, learning_rate, early_stopping, minimal, metrics, progress, checkpointing, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   1063\u001b[0m         df,\n\u001b[0;32m   1064\u001b[0m         progress_bar_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(progress),\n\u001b[0;32m   1065\u001b[0m         metrics_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics),\n\u001b[0;32m   1066\u001b[0m         checkpointing_enabled\u001b[38;5;241m=\u001b[39mcheckpointing,\n\u001b[0;32m   1067\u001b[0m         continue_training\u001b[38;5;241m=\u001b[39mcontinue_training,\n\u001b[0;32m   1068\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m     df_val, _, _, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(validation_df)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:2802\u001b[0m, in \u001b[0;36mNeuralProphet._train\u001b[1;34m(self, df, df_val, progress_bar_enabled, metrics_enabled, checkpointing_enabled, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   2800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mset_lr_finder_args(dataset_size\u001b[38;5;241m=\u001b[39mdataset_size, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;66;03m# Find suitable learning rate\u001b[39;00m\n\u001b[1;32m-> 2802\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_find(\n\u001b[0;32m   2803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   2804\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mlr_finder_args,\n\u001b[0;32m   2806\u001b[0m )\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lr_finder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;66;03m# Estimate the optimat learning rate from the loss curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:267\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    264\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mauto_lr_find \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1097\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m-> 1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1394\u001b[0m             fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:122\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_find(trainer, pl_module)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:107\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m lr_find(\n\u001b[0;32m    108\u001b[0m             trainer,\n\u001b[0;32m    109\u001b[0m             pl_module,\n\u001b[0;32m    110\u001b[0m             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_lr,\n\u001b[0;32m    111\u001b[0m             max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_lr,\n\u001b[0;32m    112\u001b[0m             num_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_training_steps,\n\u001b[0;32m    113\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode,\n\u001b[0;32m    114\u001b[0m             early_stop_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_stop_threshold,\n\u001b[0;32m    115\u001b[0m             update_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attr,\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:273\u001b[0m, in \u001b[0;36mlr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    270\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Restore initial state of model\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore(ckpt_path)\n\u001b[0;32m    274\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mremove_checkpoint(ckpt_path)\n\u001b[0;32m    275\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# reset restarting flag as checkpoint restoring sets it to True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:224\u001b[0m, in \u001b[0;36mCheckpointConnector.restore\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    state-restore, in this priority:\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        checkpoint_path: Path to a PyTorch Lightning checkpoint file.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# restore module states\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:90\u001b[0m, in \u001b[0;36mCheckpointConnector.resume_start\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 90\u001b[0m     loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:359\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    358\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\plugins\\io\\torch_io.py:86\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[1;34m(self, path, map_location)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\")\n",
    "\n",
    "data['날짜'] = pd.to_datetime(data['날짜'])  # 날짜 변환\n",
    "\n",
    "# 0인 값을 NaN으로 변경\n",
    "data.loc[data['총_결제금액'] == 0, '총_결제금액'] = np.nan\n",
    "data.loc[data['총_결제건수'] == 0, '총_결제건수'] = np.nan\n",
    "\n",
    "# 자치구별로 처리\n",
    "def fill_missing_values(group, target):\n",
    "    df = group[['날짜', target]].copy()\n",
    "    df = df.rename(columns={'날짜': 'ds', target: 'y'})\n",
    "    \n",
    "    # 학습 데이터와 예측할 데이터 분리\n",
    "    train_df = df.dropna()\n",
    "    predict_df = df[df['y'].isna()]\n",
    "    \n",
    "    if train_df.empty or predict_df.empty:\n",
    "        return group  # 학습할 데이터가 없으면 원본 반환\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = NeuralProphet()\n",
    "    model.fit(train_df, freq='D')\n",
    "    \n",
    "    # 결측치 예측\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 예측값 채우기\n",
    "    group.loc[group[target].isna(), target] = forecast['yhat1'].values\n",
    "    return group\n",
    "\n",
    "# 모든 자치구에 적용\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제금액'))\n",
    "data = data.groupby('자치구', group_keys=False).apply(lambda g: fill_missing_values(g, '총_결제건수'))\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv(\"filled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_8304\\473471435.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data.groupby(['자치구', '요일'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_8304\\473471435.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data.groupby(['자치구', '요일'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보간 완료! 결과 파일: C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_보간.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_null 제거.csv\"\n",
    "data = pd.read_csv(file_path, parse_dates=['날짜'])\n",
    "\n",
    "# 요일 컬럼 추가 (0: 월요일 ~ 6: 일요일)\n",
    "data['요일'] = data['날짜'].dt.dayofweek\n",
    "\n",
    "# 보간 대상 컬럼\n",
    "target_cols = ['총_결제금액', '총_결제건수']\n",
    "\n",
    "# 0을 NaN으로 변환\n",
    "data[target_cols] = data[target_cols].replace(0, pd.NA)\n",
    "\n",
    "# 자치구 & 요일별로 그룹화하여 보간\n",
    "for col in target_cols:\n",
    "    data[col] = data.groupby(['자치구', '요일'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
    "\n",
    "# 요일 컬럼 삭제\n",
    "data.drop(columns=['요일'], inplace=True)\n",
    "\n",
    "# 결과 저장\n",
    "output_path = r\"C:\\Users\\m\\Desktop\\머신러닝 사용 데이터\\머신러닝용_시계열데이터_보간.csv\"\n",
    "data.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"보간 완료! 결과 파일: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
