{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ëª¨ë¸ ê²€ì¦ ê²°ê³¼\n",
      "MAPE (í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): 0.0000\n",
      "RMSE (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨): 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m predicted_ratio \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future_X)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 16. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m district_total_future_sales \u001b[38;5;241m=\u001b[39m district_sales_grouped[district_sales_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mê¸°ì¤€_ë…„ë¶„ê¸°\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m future_quarter][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì´ë§¤ì¶œ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m predicted_sales \u001b[38;5;241m=\u001b[39m district_total_future_sales \u001b[38;5;241m*\u001b[39m predicted_ratio\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_district\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "base_path = \"C:/Users/m/Desktop/ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\"\n",
    "sales_data_path = f\"{base_path}/êµ¬ ë°ì´í„°/í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°/í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = f\"{base_path}/ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = f\"{base_path}/ì „ì²˜ë¦¬ ë°ì´í„°/ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = f\"{base_path}/ì „ì²˜ë¦¬ ë°ì´í„°/ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ ë³€í™˜ (20191 â†’ 2019Q1)\n",
    "sales_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = sales_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ\"].astype(str).apply(lambda x: f\"{x[:4]}Q{int(x[4])}\")\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸° (ì˜¬ë°”ë¥¸ ê°’ì´ ì…ë ¥ë  ë•Œê¹Œì§€ ë°˜ë³µ)\n",
    "while True:\n",
    "    selected_district = input(\"ì˜ˆì¸¡í•  ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "    selected_industry = input(\"ì˜ˆì¸¡í•  ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "    # 3. ì„ íƒí•œ ìƒê¶Œ-ì—…ì¢… ë°ì´í„° í•„í„°ë§\n",
    "    filtered_sales = sales_df[(sales_df[\"ìƒê¶Œ_ì½”ë“œ_ëª…\"] == selected_district) &\n",
    "                              (sales_df[\"ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…\"] == selected_industry)]\n",
    "\n",
    "    if not filtered_sales.empty:\n",
    "        break  # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜ë³µ ì¢…ë£Œ\n",
    "    else:\n",
    "        print(\"âš ï¸ ì…ë ¥í•œ ìƒê¶Œëª… ë˜ëŠ” ì—…ì¢…ëª…ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\\n\")\n",
    "\n",
    "# 4. í•´ë‹¹ ìƒê¶Œì´ ì†í•œ ìì¹˜êµ¬ ì°¾ê¸°\n",
    "selected_district_code = filtered_sales[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"].iloc[0]\n",
    "\n",
    "# 5. í•´ë‹¹ ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "district_sales = sales_df[sales_df[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"] == selected_district_code]\n",
    "\n",
    "# 6. ë¶„ê¸°ë³„ ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ í•©ì‚°\n",
    "district_sales_grouped = district_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\": \"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"}, inplace=True)\n",
    "\n",
    "# 7. í•´ë‹¹ ìƒê¶Œì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ ê³„ì‚°\n",
    "sales_with_ratio = filtered_sales.merge(district_sales_grouped, on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\")\n",
    "sales_with_ratio[\"ë§¤ì¶œ_ë¹„ìœ¨\"] = sales_with_ratio[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"] / sales_with_ratio[\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"]\n",
    "\n",
    "# 8. ìœ ë™ì¸êµ¬ ë°ì´í„° ì—°ê²° (ìì¹˜êµ¬ ê¸°ì¤€)\n",
    "pop_df.rename(columns={\"ì‹œêµ°êµ¬ëª…\": \"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", \"ì´ìƒí™œì¸êµ¬ìˆ˜\": \"ìœ ë™ì¸êµ¬\"}, inplace=True)\n",
    "sales_with_ratio = sales_with_ratio.merge(pop_df[[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", \"ìœ ë™ì¸êµ¬\"]], on=\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", how=\"left\")\n",
    "\n",
    "# 9. ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ ë° ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ì—°ê²°\n",
    "econ_df[\"ë‚ ì§œ\"] = pd.to_datetime(econ_df[\"ë‚ ì§œ\"])\n",
    "cpi_df[\"ë‚ ì§œ\"] = pd.to_datetime(cpi_df[\"ë‚ ì§œ\"])\n",
    "\n",
    "econ_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = econ_df[\"ë‚ ì§œ\"].dt.to_period(\"Q\").astype(str)\n",
    "cpi_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = cpi_df[\"ë‚ ì§œ\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "sales_with_ratio = sales_with_ratio.merge(econ_df[[\"ê¸°ì¤€_ë…„ë¶„ê¸°\", \"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\"]], on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\", how=\"left\")\n",
    "sales_with_ratio = sales_with_ratio.merge(cpi_df[[\"ê¸°ì¤€_ë…„ë¶„ê¸°\", \"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"]], on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\", how=\"left\")\n",
    "\n",
    "# 10. í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„\n",
    "features = [\"ìœ ë™ì¸êµ¬\", \"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\", \"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"]\n",
    "target = \"ë§¤ì¶œ_ë¹„ìœ¨\"\n",
    "\n",
    "sales_with_ratio.dropna(inplace=True)  # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "X = sales_with_ratio[features]\n",
    "y = sales_with_ratio[target]\n",
    "\n",
    "# 11. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 12. ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. ê²€ì¦ ë°ì´í„° í‰ê°€\n",
    "y_pred = model.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ê²€ì¦ ê²°ê³¼\")\n",
    "print(f\"MAPE (í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): {mape:.4f}\")\n",
    "print(f\"RMSE (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨): {rmse:.4f}\\n\")\n",
    "\n",
    "# 14. ì „ì²´ ë°ì´í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ í›„ 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡\n",
    "model.fit(X, y)\n",
    "\n",
    "# 15. 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ìƒì„±\n",
    "future_quarter = \"2024Q3\"\n",
    "latest_cpi = cpi_df[cpi_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"].values[0]\n",
    "latest_econ = econ_df[econ_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\"].values[0]\n",
    "latest_pop = pop_df[pop_df[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"] == selected_district_code][\"ìœ ë™ì¸êµ¬\"].mean()\n",
    "\n",
    "future_X = pd.DataFrame([[latest_pop, latest_econ, latest_cpi]], columns=features)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 16. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\n",
    "district_total_future_sales = district_sales_grouped[district_sales_grouped[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"].values[0]\n",
    "predicted_sales = district_total_future_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: {selected_district}\")\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ì—…ì¢…: {selected_industry}\")\n",
    "print(f\"ğŸ“ˆ ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ: {predicted_sales:,.0f} ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1169470658.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 39\u001b[1;36m\u001b[0m\n\u001b[1;33m    sales_ratio = filtered_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum() /\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "sales_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\êµ¬ ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "selected_district = input(\"ìì¹˜êµ¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_business = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 2019~2024ë…„ 2ë¶„ê¸°ê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§\n",
    "sales_df = sales_df[sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] <= 20242]\n",
    "filtered_sales = sales_df[(sales_df['ìƒê¶Œ_ì½”ë“œ_ëª…'] == selected_district) & \n",
    "                           (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_business)]\n",
    "\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"ì„ íƒí•œ ìƒê¶Œ-ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìì¹˜êµ¬ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°'] = sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']\n",
    "district_sales_grouped = sales_df.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'ìì¹˜êµ¬_ì´ë§¤ì¶œ'}, inplace=True)\n",
    "\n",
    "# ì„ íƒí•œ ìƒê¶Œì˜ ìì¹˜êµ¬ ì°¾ê¸°\n",
    "selected_district_code = filtered_sales[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"].iloc[0]\n",
    "district_sales_filtered = district_sales_grouped[district_sales_grouped['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district_code]\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "sales_ratio = filtered_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum() / \n",
    "district_sales_filtered.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"].sum()\n",
    "sales_ratio = sales_ratio.fillna(0).reset_index()\n",
    "sales_ratio.rename(columns={\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\": \"ë§¤ì¶œ_ë¹„ìœ¨\"}, inplace=True)\n",
    "\n",
    "# ìœ ë™ì¸êµ¬, ê²½ì œì‹¬ë¦¬ì§€ìˆ˜, ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ê²°í•©\n",
    "econ_df['ë…„ì›”'] = econ_df['ë‚ ì§œ'].str[:7]\n",
    "cpi_df['ë…„ì›”'] = cpi_df['ë‚ ì§œ'].str[:7]\n",
    "pop_df['ë…„ì›”'] = pop_df['ì‹œêµ°êµ¬ëª…'].map(lambda x: str(x)[:7])\n",
    "merged_data = sales_ratio.merge(pop_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°', right_on='ë…„ì›”', how='left')\n",
    "merged_data = merged_data.merge(econ_df, on='ë…„ì›”', how='left')\n",
    "merged_data = merged_data.merge(cpi_df, on='ë…„ì›”', how='left')\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "X = merged_data[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']]\n",
    "y = merged_data['ë§¤ì¶œ_ë¹„ìœ¨']\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# ë¯¸ë˜ ë°ì´í„° ì˜ˆì¸¡\n",
    "future_data = merged_data.iloc[-1].copy()\n",
    "future_data['ê¸°ì¤€_ë…„ë¶„ê¸°'] = 20243\n",
    "future_X = future_data[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']].values.reshape(1, -1)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\n",
    "future_quarter = 20243\n",
    "future_district_sales = district_sales_filtered.iloc[-1]['ìì¹˜êµ¬_ì´ë§¤ì¶œ'] * 1.02  # ì„±ì¥ë¥  ì ìš© (ê°€ì •ì¹˜)\n",
    "predicted_sales = future_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: {selected_district}\")\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ì—…ì¢…: {selected_business}\")\n",
    "print(f\"ğŸ“Š ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ: {predicted_sales:,.0f} ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m filtered_sales \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_sector)]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_sales\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 4. ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê³„ì‚°\u001b[39;00m\n\u001b[0;32m     28\u001b[0m district_sales_grouped \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124më‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mValueError\u001b[0m: ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\êµ¬ ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "selected_district = input(\"ìì¹˜êµ¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_sector = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 3. ì„ íƒí•œ ìƒê¶Œê³¼ ì—…ì¢… ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = df[(df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district) & (df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_sector)]\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 4. ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê³„ì‚°\n",
    "district_sales_grouped = df[df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district].groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "sector_sales_grouped = filtered_sales.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "\n",
    "# 5. ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚° ë° í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "merged_sales = pd.merge(sector_sales_grouped, district_sales_grouped, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', suffixes=('_ì—…ì¢…', '_ìì¹˜êµ¬'))\n",
    "merged_sales['ë§¤ì¶œë¹„ìœ¨'] = merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ì—…ì¢…'] / merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬']\n",
    "\n",
    "# 6. ì™¸ë¶€ ë°ì´í„° ê²°í•© (ê²½ì œì‹¬ë¦¬ì§€ìˆ˜, ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜)\n",
    "econ_df['ë‚ ì§œ'] = econ_df['ë‚ ì§œ'].astype(str).str[:6].astype(int)\n",
    "cpi_df['ë‚ ì§œ'] = cpi_df['ë‚ ì§œ'].astype(str).str[:6].astype(int)\n",
    "\n",
    "merged_sales = merged_sales.merge(econ_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', right_on='ë‚ ì§œ', how='left')\n",
    "merged_sales = merged_sales.merge(cpi_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', right_on='ë‚ ì§œ', how='left')\n",
    "merged_sales.drop(columns=['ë‚ ì§œ_x', 'ë‚ ì§œ_y'], inplace=True)\n",
    "\n",
    "# 7. í•™ìŠµ ë°ì´í„° ë° ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X = merged_sales[['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']]\n",
    "y = merged_sales['ë§¤ì¶œë¹„ìœ¨']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. ëª¨ë¸ í•™ìŠµ\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9. ëª¨ë¸ ê²€ì¦\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"ê²€ì¦ ê²°ê³¼: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
    "\n",
    "# 10. 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„\n",
    "future_quarter = 20243\n",
    "future_X = X.iloc[[-1]]  # ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 11. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ì˜ˆìƒ ë§¤ì¶œ ê³„ì‚°\n",
    "latest_district_sales = district_sales_grouped[district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20242]['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].values[0]\n",
    "predicted_district_sales = latest_district_sales * 1.02  # ìì¹˜êµ¬ ë§¤ì¶œì´ 2% ì„±ì¥í•œë‹¤ê³  ê°€ì •\n",
    "predicted_sales = predicted_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìì¹˜êµ¬: {selected_district}, ì„ íƒí•œ ì—…ì¢…: {selected_sector}\")\n",
    "print(f\"âœ… 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ì‹œêµ°êµ¬ëª…'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15336\\570041857.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ì‹œêµ°êµ¬ëª…'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ì´ìƒí™œì¸êµ¬ìˆ˜'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# ğŸ”¥ ìˆ˜ì •ëœ ë³‘í•© ì½”ë“œ: ì»¬ëŸ¼ëª…ì´ ë‹¤ë¦„ì„ ê³ ë ¤í•˜ì—¬ `left_on`, `right_on` ì‚¬ìš©\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m district_sales_grouped = district_sales_grouped.merge(\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mpopulation_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ì‹œêµ°êµ¬ëª…'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ì‹œêµ°êµ¬ëª…'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
    "sales_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\êµ¬ ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\")\n",
    "population_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\")\n",
    "economic_index_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "cpi_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "selected_district = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_industry = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 1. ì„ íƒëœ ìƒê¶Œ-ì—…ì¢…ì˜ ë§¤ì¶œ ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_df[\n",
    "    (sales_df['ìƒê¶Œ_ì½”ë“œ_ëª…'] == selected_district) & \n",
    "    (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_industry)\n",
    "]\n",
    "\n",
    "# 2. ì„ íƒëœ ìƒê¶Œì´ í¬í•¨ëœ ìì¹˜êµ¬ êµ¬í•˜ê¸°\n",
    "selected_districts = filtered_sales['ìì¹˜êµ¬_ì½”ë“œ_ëª…'].unique()\n",
    "\n",
    "# 3. í•´ë‹¹ ìì¹˜êµ¬ì™€ ì—…ì¢…ì˜ ë§¤ì¶œ ì´í•© êµ¬í•˜ê¸°\n",
    "district_sales_grouped = sales_df[\n",
    "    (sales_df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'].isin(selected_districts)) & \n",
    "    (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_industry)\n",
    "].groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "\n",
    "# 4. ì„ íƒëœ ìƒê¶Œ-ì—…ì¢…ì˜ ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "selected_total_sales = filtered_sales.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "merged_sales = selected_total_sales.merge(district_sales_grouped, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', suffixes=('_ìƒê¶Œ', '_ìì¹˜êµ¬'))\n",
    "merged_sales['ë§¤ì¶œ_ë¹„ìœ¨'] = merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìƒê¶Œ'] / merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬']\n",
    "\n",
    "# 5. ìœ ë™ì¸êµ¬ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©\n",
    "population_df = population_df.rename(columns={'ì‹œêµ°êµ¬ëª…': 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'})\n",
    "population_df = population_df.groupby('ìì¹˜êµ¬_ì½”ë“œ_ëª…')['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ğŸ”¥ ìˆ˜ì •ëœ ë³‘í•© ì½”ë“œ: ì»¬ëŸ¼ëª…ì´ ë‹¤ë¦„ì„ ê³ ë ¤í•˜ì—¬ `left_on`, `right_on` ì‚¬ìš©\n",
    "district_sales_grouped = district_sales_grouped.merge(\n",
    "    population_df, left_on='ìì¹˜êµ¬_ì½”ë“œ_ëª…', right_on='ì‹œêµ°êµ¬ëª…', how='left'\n",
    ")\n",
    "\n",
    "# 6. ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ ë° ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ë³‘í•©\n",
    "economic_index_df['ë‚ ì§œ'] = pd.to_datetime(economic_index_df['ë‚ ì§œ'])\n",
    "cpi_df['ë‚ ì§œ'] = pd.to_datetime(cpi_df['ë‚ ì§œ'])\n",
    "\n",
    "# 7. 2019~2024ë…„ 2ë¶„ê¸°ê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§\n",
    "district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(str)\n",
    "filtered_data = district_sales_grouped[district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].str[:4].astype(int) < 20243]\n",
    "\n",
    "# 8. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "features = filtered_data[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ì´ìƒí™œì¸êµ¬ìˆ˜']]\n",
    "target = filtered_data['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 10. ëª¨ë¸ í‰ê°€ ë° ë¯¸ë˜ ë§¤ì¶œ ì˜ˆì¸¡\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"ëª¨ë¸ MAE(í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae}\")\n",
    "\n",
    "# 11. 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ ì˜ˆì¸¡\n",
    "future_quarter = pd.DataFrame({'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': [20243], 'ì´ìƒí™œì¸êµ¬ìˆ˜': [population_df['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()]})\n",
    "predicted_sales = model.predict(future_quarter)\n",
    "\n",
    "# 12. ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "predicted_sales_value = predicted_sales[0] * merged_sales['ë§¤ì¶œ_ë¹„ìœ¨'].mean()\n",
    "print(f\"ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° {selected_district} {selected_industry} ë§¤ì¶œ: {predicted_sales_value:.2f} ì›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê·œ ë°ì´í„° ìƒì„±(ë§¤ì¶œ ë°ì´í„° ì¶”ì¶œ, ìœ ë™ì¸êµ¬ ë°ì´í„°ì™€ ë³‘í•©í•©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Œ 1. ë§¤ì¶œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "sales_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\êµ¬ ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "sales_df = pd.read_csv(sales_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "sales_df = sales_df[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…', 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…']]\n",
    "\n",
    "# ğŸ“Œ 2. ìœ ë™ì¸êµ¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "pop_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "pop_df = pd.read_csv(pop_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ê¸°ì¤€ì¼IDë¥¼ ì—°ë¶„ê¸° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 20180416 -> 20181)\n",
    "pop_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = (pop_df['ê¸°ì¤€ì¼ID'] // 10000) * 10 + ((pop_df['ê¸°ì¤€ì¼ID'] % 10000) // 3000 + 1)\n",
    "\n",
    "# ì¼ë³„ ë°ì´í„°ë¥¼ ë¶„ê¸°ë³„ë¡œ ì§‘ê³„\n",
    "pop_quarterly_df = pop_df.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ì‹œêµ°êµ¬ëª…'], as_index=False)[\n",
    "    ['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']\n",
    "].sum()\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½ (ë§¤ì¶œ ë°ì´í„°ì™€ ì¼ê´€ì„± ìœ ì§€)\n",
    "pop_quarterly_df = pop_quarterly_df.rename(columns={'ì‹œêµ°êµ¬ëª…': 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'})\n",
    "\n",
    "# ğŸ“Œ 3. ë°ì´í„° ë³‘í•©\n",
    "merged_df = sales_df.merge(pop_quarterly_df, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], how='left')\n",
    "\n",
    "# ğŸ“Œ 4. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\"\n",
    "merged_df.to_csv(output_file, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "print(f\"ë°ì´í„° ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 101649046.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + (date.month - 1) // 3 + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[(sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "                            (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], suffixes=('', '_ìì¹˜êµ¬ì´í•©'))\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "X = filtered_sales[['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_features = [[q3_cpi, q3_previous_qtr_avg_esi, \n",
    "                   filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]]\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 101649046.18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[(sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "                            (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], \n",
    "                                      suffixes=('', '_ìì¹˜êµ¬ì´í•©'))\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "feature_columns = ['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', \n",
    "                   'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_data = {\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜': [q3_cpi],\n",
    "    'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜': [q3_previous_qtr_avg_esi],\n",
    "    'ì´ìƒí™œì¸êµ¬ìˆ˜': [filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœì†Œì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜': [filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„±í•˜ì—¬ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ feature ì´ë¦„ì„ ë™ì¼í•˜ê²Œ ì§€ì •\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë¥¸ ëª¨ë¸(XGboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.6/124.9 MB 39.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 13.1/124.9 MB 32.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 19.7/124.9 MB 33.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 24.9/124.9 MB 30.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 30.4/124.9 MB 30.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 29.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 39.3/124.9 MB 27.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.8/124.9 MB 27.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 49.8/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 59.8/124.9 MB 26.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.0/124.9 MB 26.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 68.7/124.9 MB 25.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 71.8/124.9 MB 25.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.5/124.9 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.4/124.9 MB 24.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 82.1/124.9 MB 23.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 84.9/124.9 MB 23.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 87.8/124.9 MB 22.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 92.3/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 97.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 102.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.5/124.9 MB 23.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.7/124.9 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.0/124.9 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 23.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 22.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 12164253.54, RMSE: 14829197.65\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 105060776.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[\n",
    "    (sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "    (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)\n",
    "]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(\n",
    "    region_sales, \n",
    "    on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], \n",
    "    suffixes=('', '_ìì¹˜êµ¬ì´í•©')\n",
    ")\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "feature_columns = [\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', \n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'\n",
    "]\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost ëª¨ë¸ ì ìš© (ëœë¤í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ)\n",
    "model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[\n",
    "    esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code\n",
    "]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_data = {\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜': [q3_cpi],\n",
    "    'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜': [q3_previous_qtr_avg_esi],\n",
    "    'ì´ìƒí™œì¸êµ¬ìˆ˜': [filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœì†Œì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜': [filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„±í•˜ì—¬ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ feature ì´ë¦„ì„ ë™ì¼í•˜ê²Œ ì§€ì •\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë¥¸ ë°ì´í„° ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neural\n",
      "  Downloading neural-0.1.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: prophet in c:\\users\\m\\anaconda3\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (3.9.2)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (2.2.2)\n",
      "Requirement already satisfied: holidays<1,>=0.25 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (0.66)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (4.66.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\m\\anaconda3\\lib\\site-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\m\\anaconda3\\lib\\site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n",
      "Downloading neural-0.1.0-py2.py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: neural\n",
      "Successfully installed neural-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neural prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralProphet\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neuralprophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\")\n",
    "\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])  # ë‚ ì§œ ë³€í™˜\n",
    "\n",
    "# 0ì¸ ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½\n",
    "data.loc[data['ì´_ê²°ì œê¸ˆì•¡'] == 0, 'ì´_ê²°ì œê¸ˆì•¡'] = np.nan\n",
    "data.loc[data['ì´_ê²°ì œê±´ìˆ˜'] == 0, 'ì´_ê²°ì œê±´ìˆ˜'] = np.nan\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ë¡œ ì²˜ë¦¬\n",
    "def fill_missing_values(group, target):\n",
    "    df = group[['ë‚ ì§œ', target]].copy()\n",
    "    df = df.rename(columns={'ë‚ ì§œ': 'ds', target: 'y'})\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„°ì™€ ì˜ˆì¸¡í•  ë°ì´í„° ë¶„ë¦¬\n",
    "    train_df = df.dropna()\n",
    "    predict_df = df[df['y'].isna()]\n",
    "    \n",
    "    if train_df.empty or predict_df.empty:\n",
    "        return group  # í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model = NeuralProphet()\n",
    "    model.fit(train_df, freq='D', progress_bar=False)\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ ì±„ìš°ê¸°\n",
    "    group.loc[group[target].isna(), target] = forecast['yhat1'].values\n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "data.to_csv(\"filled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
