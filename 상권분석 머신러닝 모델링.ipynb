{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ëª¨ë¸ ê²€ì¦ ê²°ê³¼\n",
      "MAPE (í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): 0.0000\n",
      "RMSE (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨): 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m predicted_ratio \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future_X)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 16. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m district_total_future_sales \u001b[38;5;241m=\u001b[39m district_sales_grouped[district_sales_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mê¸°ì¤€_ë…„ë¶„ê¸°\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m future_quarter][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì´ë§¤ì¶œ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m predicted_sales \u001b[38;5;241m=\u001b[39m district_total_future_sales \u001b[38;5;241m*\u001b[39m predicted_ratio\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_district\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "base_path = \"C:/Users/m/Desktop/ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\"\n",
    "sales_data_path = f\"{base_path}/êµ¬ ë°ì´í„°/í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°/í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = f\"{base_path}/ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = f\"{base_path}/ì „ì²˜ë¦¬ ë°ì´í„°/ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = f\"{base_path}/ì „ì²˜ë¦¬ ë°ì´í„°/ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ ë³€í™˜ (20191 â†’ 2019Q1)\n",
    "sales_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = sales_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ\"].astype(str).apply(lambda x: f\"{x[:4]}Q{int(x[4])}\")\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸° (ì˜¬ë°”ë¥¸ ê°’ì´ ì…ë ¥ë  ë•Œê¹Œì§€ ë°˜ë³µ)\n",
    "while True:\n",
    "    selected_district = input(\"ì˜ˆì¸¡í•  ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "    selected_industry = input(\"ì˜ˆì¸¡í•  ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "    # 3. ì„ íƒí•œ ìƒê¶Œ-ì—…ì¢… ë°ì´í„° í•„í„°ë§\n",
    "    filtered_sales = sales_df[(sales_df[\"ìƒê¶Œ_ì½”ë“œ_ëª…\"] == selected_district) &\n",
    "                              (sales_df[\"ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…\"] == selected_industry)]\n",
    "\n",
    "    if not filtered_sales.empty:\n",
    "        break  # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜ë³µ ì¢…ë£Œ\n",
    "    else:\n",
    "        print(\"âš ï¸ ì…ë ¥í•œ ìƒê¶Œëª… ë˜ëŠ” ì—…ì¢…ëª…ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.\\n\")\n",
    "\n",
    "# 4. í•´ë‹¹ ìƒê¶Œì´ ì†í•œ ìì¹˜êµ¬ ì°¾ê¸°\n",
    "selected_district_code = filtered_sales[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"].iloc[0]\n",
    "\n",
    "# 5. í•´ë‹¹ ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "district_sales = sales_df[sales_df[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"] == selected_district_code]\n",
    "\n",
    "# 6. ë¶„ê¸°ë³„ ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ í•©ì‚°\n",
    "district_sales_grouped = district_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\": \"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"}, inplace=True)\n",
    "\n",
    "# 7. í•´ë‹¹ ìƒê¶Œì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ ê³„ì‚°\n",
    "sales_with_ratio = filtered_sales.merge(district_sales_grouped, on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\")\n",
    "sales_with_ratio[\"ë§¤ì¶œ_ë¹„ìœ¨\"] = sales_with_ratio[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"] / sales_with_ratio[\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"]\n",
    "\n",
    "# 8. ìœ ë™ì¸êµ¬ ë°ì´í„° ì—°ê²° (ìì¹˜êµ¬ ê¸°ì¤€)\n",
    "pop_df.rename(columns={\"ì‹œêµ°êµ¬ëª…\": \"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", \"ì´ìƒí™œì¸êµ¬ìˆ˜\": \"ìœ ë™ì¸êµ¬\"}, inplace=True)\n",
    "sales_with_ratio = sales_with_ratio.merge(pop_df[[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", \"ìœ ë™ì¸êµ¬\"]], on=\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\", how=\"left\")\n",
    "\n",
    "# 9. ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ ë° ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ì—°ê²°\n",
    "econ_df[\"ë‚ ì§œ\"] = pd.to_datetime(econ_df[\"ë‚ ì§œ\"])\n",
    "cpi_df[\"ë‚ ì§œ\"] = pd.to_datetime(cpi_df[\"ë‚ ì§œ\"])\n",
    "\n",
    "econ_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = econ_df[\"ë‚ ì§œ\"].dt.to_period(\"Q\").astype(str)\n",
    "cpi_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] = cpi_df[\"ë‚ ì§œ\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "sales_with_ratio = sales_with_ratio.merge(econ_df[[\"ê¸°ì¤€_ë…„ë¶„ê¸°\", \"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\"]], on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\", how=\"left\")\n",
    "sales_with_ratio = sales_with_ratio.merge(cpi_df[[\"ê¸°ì¤€_ë…„ë¶„ê¸°\", \"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"]], on=\"ê¸°ì¤€_ë…„ë¶„ê¸°\", how=\"left\")\n",
    "\n",
    "# 10. í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„\n",
    "features = [\"ìœ ë™ì¸êµ¬\", \"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\", \"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"]\n",
    "target = \"ë§¤ì¶œ_ë¹„ìœ¨\"\n",
    "\n",
    "sales_with_ratio.dropna(inplace=True)  # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "X = sales_with_ratio[features]\n",
    "y = sales_with_ratio[target]\n",
    "\n",
    "# 11. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 12. ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. ê²€ì¦ ë°ì´í„° í‰ê°€\n",
    "y_pred = model.predict(X_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ê²€ì¦ ê²°ê³¼\")\n",
    "print(f\"MAPE (í‰ê·  ì ˆëŒ€ í¼ì„¼íŠ¸ ì˜¤ì°¨): {mape:.4f}\")\n",
    "print(f\"RMSE (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨): {rmse:.4f}\\n\")\n",
    "\n",
    "# 14. ì „ì²´ ë°ì´í„°ë¡œ ë‹¤ì‹œ í•™ìŠµ í›„ 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡\n",
    "model.fit(X, y)\n",
    "\n",
    "# 15. 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ìƒì„±\n",
    "future_quarter = \"2024Q3\"\n",
    "latest_cpi = cpi_df[cpi_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜\"].values[0]\n",
    "latest_econ = econ_df[econ_df[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ê²½ì œì‹¬ë¦¬ì§€ìˆ˜\"].values[0]\n",
    "latest_pop = pop_df[pop_df[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"] == selected_district_code][\"ìœ ë™ì¸êµ¬\"].mean()\n",
    "\n",
    "future_X = pd.DataFrame([[latest_pop, latest_econ, latest_cpi]], columns=features)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 16. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\n",
    "district_total_future_sales = district_sales_grouped[district_sales_grouped[\"ê¸°ì¤€_ë…„ë¶„ê¸°\"] == future_quarter][\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"].values[0]\n",
    "predicted_sales = district_total_future_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: {selected_district}\")\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ì—…ì¢…: {selected_industry}\")\n",
    "print(f\"ğŸ“ˆ ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ: {predicted_sales:,.0f} ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1169470658.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 39\u001b[1;36m\u001b[0m\n\u001b[1;33m    sales_ratio = filtered_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum() /\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "sales_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\êµ¬ ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_df = pd.read_csv(sales_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "selected_district = input(\"ìì¹˜êµ¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_business = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 2019~2024ë…„ 2ë¶„ê¸°ê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§\n",
    "sales_df = sales_df[sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] <= 20242]\n",
    "filtered_sales = sales_df[(sales_df['ìƒê¶Œ_ì½”ë“œ_ëª…'] == selected_district) & \n",
    "                           (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_business)]\n",
    "\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"ì„ íƒí•œ ìƒê¶Œ-ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìì¹˜êµ¬ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°'] = sales_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ']\n",
    "district_sales_grouped = sales_df.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "district_sales_grouped.rename(columns={'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'ìì¹˜êµ¬_ì´ë§¤ì¶œ'}, inplace=True)\n",
    "\n",
    "# ì„ íƒí•œ ìƒê¶Œì˜ ìì¹˜êµ¬ ì°¾ê¸°\n",
    "selected_district_code = filtered_sales[\"ìì¹˜êµ¬_ì½”ë“œ_ëª…\"].iloc[0]\n",
    "district_sales_filtered = district_sales_grouped[district_sales_grouped['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district_code]\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "sales_ratio = filtered_sales.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\"].sum() / \n",
    "district_sales_filtered.groupby(\"ê¸°ì¤€_ë…„ë¶„ê¸°\")[\"ìì¹˜êµ¬_ì´ë§¤ì¶œ\"].sum()\n",
    "sales_ratio = sales_ratio.fillna(0).reset_index()\n",
    "sales_ratio.rename(columns={\"ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\": \"ë§¤ì¶œ_ë¹„ìœ¨\"}, inplace=True)\n",
    "\n",
    "# ìœ ë™ì¸êµ¬, ê²½ì œì‹¬ë¦¬ì§€ìˆ˜, ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ê²°í•©\n",
    "econ_df['ë…„ì›”'] = econ_df['ë‚ ì§œ'].str[:7]\n",
    "cpi_df['ë…„ì›”'] = cpi_df['ë‚ ì§œ'].str[:7]\n",
    "pop_df['ë…„ì›”'] = pop_df['ì‹œêµ°êµ¬ëª…'].map(lambda x: str(x)[:7])\n",
    "merged_data = sales_ratio.merge(pop_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°', right_on='ë…„ì›”', how='left')\n",
    "merged_data = merged_data.merge(econ_df, on='ë…„ì›”', how='left')\n",
    "merged_data = merged_data.merge(cpi_df, on='ë…„ì›”', how='left')\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "X = merged_data[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']]\n",
    "y = merged_data['ë§¤ì¶œ_ë¹„ìœ¨']\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# ë¯¸ë˜ ë°ì´í„° ì˜ˆì¸¡\n",
    "future_data = merged_data.iloc[-1].copy()\n",
    "future_data['ê¸°ì¤€_ë…„ë¶„ê¸°'] = 20243\n",
    "future_X = future_data[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']].values.reshape(1, -1)\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ë§¤ì¶œ ì˜ˆì¸¡ í›„ ìƒê¶Œë³„ ì ìš©\n",
    "future_quarter = 20243\n",
    "future_district_sales = district_sales_filtered.iloc[-1]['ìì¹˜êµ¬_ì´ë§¤ì¶œ'] * 1.02  # ì„±ì¥ë¥  ì ìš© (ê°€ì •ì¹˜)\n",
    "predicted_sales = future_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìƒê¶Œ: {selected_district}\")\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ì—…ì¢…: {selected_business}\")\n",
    "print(f\"ğŸ“Š ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ: {predicted_sales:,.0f} ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m filtered_sales \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_sector)]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_sales\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 4. ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê³„ì‚°\u001b[39;00m\n\u001b[0;32m     28\u001b[0m district_sales_grouped \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬_ì½”ë“œ_ëª…\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_district]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124më‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mValueError\u001b[0m: ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\êµ¬ ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "pop_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "econ_index_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\"\n",
    "cpi_data_path = \"C:\\\\Users\\\\m\\\\Desktop\\\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\\\ì „ì²˜ë¦¬ ë°ì´í„°\\\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "pop_df = pd.read_csv(pop_data_path)\n",
    "econ_df = pd.read_csv(econ_index_path)\n",
    "cpi_df = pd.read_csv(cpi_data_path)\n",
    "\n",
    "# 2. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "selected_district = input(\"ìì¹˜êµ¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_sector = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 3. ì„ íƒí•œ ìƒê¶Œê³¼ ì—…ì¢… ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = df[(df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district) & (df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_sector)]\n",
    "if filtered_sales.empty:\n",
    "    raise ValueError(\"ì„ íƒí•œ ìì¹˜êµ¬ì™€ ì—…ì¢… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 4. ìì¹˜êµ¬ ì „ì²´ ë§¤ì¶œ ë°ì´í„° ê³„ì‚°\n",
    "district_sales_grouped = df[df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'] == selected_district].groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "sector_sales_grouped = filtered_sales.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "\n",
    "# 5. ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚° ë° í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "merged_sales = pd.merge(sector_sales_grouped, district_sales_grouped, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', suffixes=('_ì—…ì¢…', '_ìì¹˜êµ¬'))\n",
    "merged_sales['ë§¤ì¶œë¹„ìœ¨'] = merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ì—…ì¢…'] / merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬']\n",
    "\n",
    "# 6. ì™¸ë¶€ ë°ì´í„° ê²°í•© (ê²½ì œì‹¬ë¦¬ì§€ìˆ˜, ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜)\n",
    "econ_df['ë‚ ì§œ'] = econ_df['ë‚ ì§œ'].astype(str).str[:6].astype(int)\n",
    "cpi_df['ë‚ ì§œ'] = cpi_df['ë‚ ì§œ'].astype(str).str[:6].astype(int)\n",
    "\n",
    "merged_sales = merged_sales.merge(econ_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', right_on='ë‚ ì§œ', how='left')\n",
    "merged_sales = merged_sales.merge(cpi_df, left_on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', right_on='ë‚ ì§œ', how='left')\n",
    "merged_sales.drop(columns=['ë‚ ì§œ_x', 'ë‚ ì§œ_y'], inplace=True)\n",
    "\n",
    "# 7. í•™ìŠµ ë°ì´í„° ë° ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "X = merged_sales[['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']]\n",
    "y = merged_sales['ë§¤ì¶œë¹„ìœ¨']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. ëª¨ë¸ í•™ìŠµ\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9. ëª¨ë¸ ê²€ì¦\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"ê²€ì¦ ê²°ê³¼: MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
    "\n",
    "# 10. 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„\n",
    "future_quarter = 20243\n",
    "future_X = X.iloc[[-1]]  # ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "predicted_ratio = model.predict(future_X)[0]\n",
    "\n",
    "# 11. 2024ë…„ 3ë¶„ê¸° ìì¹˜êµ¬ ì˜ˆìƒ ë§¤ì¶œ ê³„ì‚°\n",
    "latest_district_sales = district_sales_grouped[district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20242]['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].values[0]\n",
    "predicted_district_sales = latest_district_sales * 1.02  # ìì¹˜êµ¬ ë§¤ì¶œì´ 2% ì„±ì¥í•œë‹¤ê³  ê°€ì •\n",
    "predicted_sales = predicted_district_sales * predicted_ratio\n",
    "\n",
    "print(f\"ğŸ“Œ ì„ íƒí•œ ìì¹˜êµ¬: {selected_district}, ì„ íƒí•œ ì—…ì¢…: {selected_sector}\")\n",
    "print(f\"âœ… 2024ë…„ 3ë¶„ê¸° ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ì‹œêµ°êµ¬ëª…'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15336\\570041857.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ì‹œêµ°êµ¬ëª…'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpopulation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ì´ìƒí™œì¸êµ¬ìˆ˜'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# ğŸ”¥ ìˆ˜ì •ëœ ë³‘í•© ì½”ë“œ: ì»¬ëŸ¼ëª…ì´ ë‹¤ë¦„ì„ ê³ ë ¤í•˜ì—¬ `left_on`, `right_on` ì‚¬ìš©\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m district_sales_grouped = district_sales_grouped.merge(\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mpopulation_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ìì¹˜êµ¬_ì½”ë“œ_ëª…'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ì‹œêµ°êµ¬ëª…'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ì‹œêµ°êµ¬ëª…'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
    "sales_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\êµ¬ ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\")\n",
    "population_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\")\n",
    "economic_index_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "cpi_df = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "selected_district = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "selected_industry = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# 1. ì„ íƒëœ ìƒê¶Œ-ì—…ì¢…ì˜ ë§¤ì¶œ ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_df[\n",
    "    (sales_df['ìƒê¶Œ_ì½”ë“œ_ëª…'] == selected_district) & \n",
    "    (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_industry)\n",
    "]\n",
    "\n",
    "# 2. ì„ íƒëœ ìƒê¶Œì´ í¬í•¨ëœ ìì¹˜êµ¬ êµ¬í•˜ê¸°\n",
    "selected_districts = filtered_sales['ìì¹˜êµ¬_ì½”ë“œ_ëª…'].unique()\n",
    "\n",
    "# 3. í•´ë‹¹ ìì¹˜êµ¬ì™€ ì—…ì¢…ì˜ ë§¤ì¶œ ì´í•© êµ¬í•˜ê¸°\n",
    "district_sales_grouped = sales_df[\n",
    "    (sales_df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'].isin(selected_districts)) & \n",
    "    (sales_df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == selected_industry)\n",
    "].groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "\n",
    "# 4. ì„ íƒëœ ìƒê¶Œ-ì—…ì¢…ì˜ ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "selected_total_sales = filtered_sales.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "merged_sales = selected_total_sales.merge(district_sales_grouped, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', suffixes=('_ìƒê¶Œ', '_ìì¹˜êµ¬'))\n",
    "merged_sales['ë§¤ì¶œ_ë¹„ìœ¨'] = merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìƒê¶Œ'] / merged_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬']\n",
    "\n",
    "# 5. ìœ ë™ì¸êµ¬ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©\n",
    "population_df = population_df.rename(columns={'ì‹œêµ°êµ¬ëª…': 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'})\n",
    "population_df = population_df.groupby('ìì¹˜êµ¬_ì½”ë“œ_ëª…')['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ğŸ”¥ ìˆ˜ì •ëœ ë³‘í•© ì½”ë“œ: ì»¬ëŸ¼ëª…ì´ ë‹¤ë¦„ì„ ê³ ë ¤í•˜ì—¬ `left_on`, `right_on` ì‚¬ìš©\n",
    "district_sales_grouped = district_sales_grouped.merge(\n",
    "    population_df, left_on='ìì¹˜êµ¬_ì½”ë“œ_ëª…', right_on='ì‹œêµ°êµ¬ëª…', how='left'\n",
    ")\n",
    "\n",
    "# 6. ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ ë° ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ë³‘í•©\n",
    "economic_index_df['ë‚ ì§œ'] = pd.to_datetime(economic_index_df['ë‚ ì§œ'])\n",
    "cpi_df['ë‚ ì§œ'] = pd.to_datetime(cpi_df['ë‚ ì§œ'])\n",
    "\n",
    "# 7. 2019~2024ë…„ 2ë¶„ê¸°ê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§\n",
    "district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(str)\n",
    "filtered_data = district_sales_grouped[district_sales_grouped['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].str[:4].astype(int) < 20243]\n",
    "\n",
    "# 8. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "features = filtered_data[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ì´ìƒí™œì¸êµ¬ìˆ˜']]\n",
    "target = filtered_data['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 10. ëª¨ë¸ í‰ê°€ ë° ë¯¸ë˜ ë§¤ì¶œ ì˜ˆì¸¡\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"ëª¨ë¸ MAE(í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae}\")\n",
    "\n",
    "# 11. 2024ë…„ 3ë¶„ê¸° ë§¤ì¶œ ì˜ˆì¸¡\n",
    "future_quarter = pd.DataFrame({'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': [20243], 'ì´ìƒí™œì¸êµ¬ìˆ˜': [population_df['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()]})\n",
    "predicted_sales = model.predict(future_quarter)\n",
    "\n",
    "# 12. ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "predicted_sales_value = predicted_sales[0] * merged_sales['ë§¤ì¶œ_ë¹„ìœ¨'].mean()\n",
    "print(f\"ì˜ˆì¸¡ëœ 2024ë…„ 3ë¶„ê¸° {selected_district} {selected_industry} ë§¤ì¶œ: {predicted_sales_value:.2f} ì›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê·œ ë°ì´í„° ìƒì„±(ë§¤ì¶œ ë°ì´í„° ì¶”ì¶œ, ìœ ë™ì¸êµ¬ ë°ì´í„°ì™€ ë³‘í•©í•©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Œ 1. ë§¤ì¶œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "sales_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\êµ¬ ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§í•œí†µí•©ë°ì´í„°\\í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì¼ë‹¨ë‹¤í•œí†µí•©ë°ì´í„°.csv\"\n",
    "sales_df = pd.read_csv(sales_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "sales_df = sales_df[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…', 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…']]\n",
    "\n",
    "# ğŸ“Œ 2. ìœ ë™ì¸êµ¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "pop_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì„œìš¸ ì¼ë³„ ìœ ë™ì¸êµ¬.csv\"\n",
    "pop_df = pd.read_csv(pop_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ê¸°ì¤€ì¼IDë¥¼ ì—°ë¶„ê¸° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 20180416 -> 20181)\n",
    "pop_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = (pop_df['ê¸°ì¤€ì¼ID'] // 10000) * 10 + ((pop_df['ê¸°ì¤€ì¼ID'] % 10000) // 3000 + 1)\n",
    "\n",
    "# ì¼ë³„ ë°ì´í„°ë¥¼ ë¶„ê¸°ë³„ë¡œ ì§‘ê³„\n",
    "pop_quarterly_df = pop_df.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ì‹œêµ°êµ¬ëª…'], as_index=False)[\n",
    "    ['ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']\n",
    "].sum()\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½ (ë§¤ì¶œ ë°ì´í„°ì™€ ì¼ê´€ì„± ìœ ì§€)\n",
    "pop_quarterly_df = pop_quarterly_df.rename(columns={'ì‹œêµ°êµ¬ëª…': 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'})\n",
    "\n",
    "# ğŸ“Œ 3. ë°ì´í„° ë³‘í•©\n",
    "merged_df = sales_df.merge(pop_quarterly_df, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], how='left')\n",
    "\n",
    "# ğŸ“Œ 4. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\"\n",
    "merged_df.to_csv(output_file, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "print(f\"ë°ì´í„° ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 101649046.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + (date.month - 1) // 3 + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[(sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "                            (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], suffixes=('', '_ìì¹˜êµ¬ì´í•©'))\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "X = filtered_sales[['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_features = [[q3_cpi, q3_previous_qtr_avg_esi, \n",
    "                   filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean(), \n",
    "                   filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]]\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 9829827.11, RMSE: 14001143.83\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 101649046.18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[(sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "                            (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(region_sales, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], \n",
    "                                      suffixes=('', '_ìì¹˜êµ¬ì´í•©'))\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "feature_columns = ['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', \n",
    "                   'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜']\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_data = {\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜': [q3_cpi],\n",
    "    'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜': [q3_previous_qtr_avg_esi],\n",
    "    'ì´ìƒí™œì¸êµ¬ìˆ˜': [filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœì†Œì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜': [filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„±í•˜ì—¬ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ feature ì´ë¦„ì„ ë™ì¼í•˜ê²Œ ì§€ì •\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë¥¸ ëª¨ë¸(XGboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\m\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.6/124.9 MB 39.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 13.1/124.9 MB 32.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 19.7/124.9 MB 33.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 24.9/124.9 MB 30.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 30.4/124.9 MB 30.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 29.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 39.3/124.9 MB 27.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.8/124.9 MB 27.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 49.8/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 59.8/124.9 MB 26.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.0/124.9 MB 26.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 68.7/124.9 MB 25.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 71.8/124.9 MB 25.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.5/124.9 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.4/124.9 MB 24.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 82.1/124.9 MB 23.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 84.9/124.9 MB 23.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 87.8/124.9 MB 22.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 92.3/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 97.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 102.0/124.9 MB 22.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.5/124.9 MB 23.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.7/124.9 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.0/124.9 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 23.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 22.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ê²°ê³¼ - MAE: 12164253.54, RMSE: 14829197.65\n",
      "2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: 105060776.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "sales_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë³‘í•©ëœ_ë§¤ì¶œ_ìœ ë™ì¸êµ¬.csv\")\n",
    "cpi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜.csv\")\n",
    "esi_data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ì „ì²˜ë¦¬ ë°ì´í„°\\ê²½ì œì‹¬ë¦¬ì§€ìˆ˜.csv\")\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "cpi_data['ë‚ ì§œ'] = pd.to_datetime(cpi_data['ë‚ ì§œ'])\n",
    "esi_data['ë‚ ì§œ'] = pd.to_datetime(esi_data['ë‚ ì§œ'])\n",
    "\n",
    "# ë¶„ê¸° ì½”ë“œë¡œ ë³€í™˜\n",
    "def get_quarter_code(date):\n",
    "    return date.year * 10 + ((date.month - 1) // 3) + 1\n",
    "\n",
    "cpi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = cpi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "esi_data['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] = esi_data['ë‚ ì§œ'].apply(get_quarter_code)\n",
    "\n",
    "# ë¶„ê¸°ë³„ í‰ê·  ê³„ì‚°\n",
    "cpi_quarterly = cpi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].mean().reset_index()\n",
    "esi_quarterly = esi_data.groupby('ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ')['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].mean().reset_index()\n",
    "\n",
    "# ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ëŠ” ì§ì „ ë¶„ê¸°ì˜ í‰ê· ê°’ ì‚¬ìš©\n",
    "esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'] = esi_quarterly['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].shift(1)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ê°’ ë°›ê¸°\n",
    "region_name = input(\"ìƒê¶Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "industry_name = input(\"ì—…ì¢…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# ìƒê¶Œ ë° ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§\n",
    "filtered_sales = sales_data[\n",
    "    (sales_data['ìƒê¶Œ_ì½”ë“œ_ëª…'] == region_name) & \n",
    "    (sales_data['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'] == industry_name)\n",
    "]\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë§¤ì¶œ ì´í•© ê³„ì‚°\n",
    "region_sales = sales_data.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].sum().reset_index()\n",
    "filtered_sales = filtered_sales.merge(\n",
    "    region_sales, \n",
    "    on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…'], \n",
    "    suffixes=('', '_ìì¹˜êµ¬ì´í•©')\n",
    ")\n",
    "\n",
    "# ë§¤ì¶œ ë¹„ìœ¨ ê³„ì‚°\n",
    "filtered_sales['ë§¤ì¶œë¹„ìœ¨'] = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_ìì¹˜êµ¬ì´í•©']\n",
    "\n",
    "# ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ë³‘í•©\n",
    "filtered_sales = filtered_sales.merge(cpi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "filtered_sales = filtered_sales.merge(esi_quarterly, on='ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', how='left')\n",
    "\n",
    "# í•™ìŠµ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •\n",
    "feature_columns = [\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜', \n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜', 'ì¼ìµœì†Œì¸êµ¬ìˆ˜', 'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜', 'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'\n",
    "]\n",
    "X = filtered_sales[feature_columns]\n",
    "y = filtered_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']\n",
    "\n",
    "# í•™ìŠµ-ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2 ë¹„ìœ¨)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost ëª¨ë¸ ì ìš© (ëœë¤í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ)\n",
    "model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"ê²€ì¦ ê²°ê³¼ - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 2024ë…„ 3ë¶„ê¸°ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ì™€ ê²½ì œì‹¬ë¦¬ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "q3_cpi = cpi_quarterly[cpi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20243]['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜'].values[0]\n",
    "q3_previous_qtr_code = 20242  # ì§ì „ ë¶„ê¸° ì½”ë“œ\n",
    "q3_previous_qtr_avg_esi = esi_quarterly[\n",
    "    esi_quarterly['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == q3_previous_qtr_code\n",
    "]['ê²½ì œì‹¬ë¦¬ì§€ìˆ˜'].values[0]\n",
    "\n",
    "# ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ê°’ ìƒì„± (ì„ì˜ë¡œ ì¸êµ¬ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì„¤ì •)\n",
    "input_data = {\n",
    "    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜': [q3_cpi],\n",
    "    'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜': [q3_previous_qtr_avg_esi],\n",
    "    'ì´ìƒí™œì¸êµ¬ìˆ˜': [filtered_sales['ì´ìƒí™œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœì†Œì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœì†Œì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜': [filtered_sales['ì¼ìµœëŒ€ì´ë™ì¸êµ¬ìˆ˜'].mean()],\n",
    "    'ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜': [filtered_sales['ì„œìš¸ì™¸ìœ ì…ì¸êµ¬ìˆ˜'].mean()]\n",
    "}\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„±í•˜ì—¬ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ feature ì´ë¦„ì„ ë™ì¼í•˜ê²Œ ì§€ì •\n",
    "input_features = pd.DataFrame(input_data, columns=feature_columns)\n",
    "\n",
    "predicted_sales_q3 = model.predict(input_features)[0]\n",
    "print(f\"2024ë…„ 3ë¶„ê¸°ì˜ ì˜ˆìƒ ë§¤ì¶œ: {predicted_sales_q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë¥¸ ë°ì´í„° ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralprophet\n",
      "  Downloading neuralprophet-0.8.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting captum>=0.6.0 (from neuralprophet)\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: holidays>=0.41 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.66)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.5.3 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (3.9.2)\n",
      "Requirement already satisfied: nbformat<6.0.0,>=5.8.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.10.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.2.2)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.24.1)\n",
      "Collecting pytorch-lightning<2.0.0,>=1.9.4 (from neuralprophet)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: tensorboard<3.0.0,>=2.11.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.18.0)\n",
      "Collecting torch<3.0.0,>=2.0.0 (from neuralprophet)\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchmetrics<2.0.0,>=1.0.0 (from neuralprophet)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (4.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\m\\anaconda3\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\m\\anaconda3\\lib\\site-packages (from holidays>=0.41->neuralprophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (3.1.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from plotly<6.0.0,>=5.13.1->neuralprophet) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2024.6.1)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet)\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch<3.0.0,>=2.0.0->neuralprophet)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->neuralprophet) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\anaconda3\\lib\\site-packages (from tqdm->captum>=0.6.0->neuralprophet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.7)\n",
      "Downloading neuralprophet-0.8.0-py3-none-any.whl (145 kB)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "   ---------------------------------------- 0.0/829.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 829.5/829.5 kB 35.5 MB/s eta 0:00:00\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 7.9/204.1 MB 40.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 18.6/204.1 MB 46.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 29.4/204.1 MB 49.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 40.1/204.1 MB 50.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 49.8/204.1 MB 49.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 59.5/204.1 MB 49.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 67.9/204.1 MB 48.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 77.6/204.1 MB 48.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 85.5/204.1 MB 47.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 93.8/204.1 MB 46.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 101.4/204.1 MB 45.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 107.5/204.1 MB 44.3 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.1/204.1 MB 44.1 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 124.3/204.1 MB 43.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 132.6/204.1 MB 43.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 140.5/204.1 MB 43.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 148.9/204.1 MB 43.0 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 158.1/204.1 MB 43.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 161.0/204.1 MB 41.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 168.0/204.1 MB 41.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 175.4/204.1 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 182.2/204.1 MB 40.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.7/204.1 MB 40.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 192.2/204.1 MB 39.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.8/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.6/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "   ---------------------------------------- 0.0/927.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 927.3/927.3 kB 21.5 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: sympy, lightning-utilities, torch, torchmetrics, captum, pytorch-lightning, neuralprophet\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed captum-0.7.0 lightning-utilities-0.12.0 neuralprophet-0.8.0 pytorch-lightning-1.9.5 sympy-1.13.1 torch-2.6.0 torchmetrics-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neuralprophet in c:\\users\\m\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: captum>=0.6.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.7.0)\n",
      "Requirement already satisfied: holidays>=0.41 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (0.66)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.5.3 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (3.9.2)\n",
      "Requirement already satisfied: nbformat<6.0.0,>=5.8.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.10.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.2.2)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (5.24.1)\n",
      "Requirement already satisfied: pytorch-lightning<2.0.0,>=1.9.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.9.5)\n",
      "Requirement already satisfied: tensorboard<3.0.0,>=2.11.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.18.0)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (2.6.0)\n",
      "Requirement already satisfied: torchmetrics<2.0.0,>=1.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (1.6.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from neuralprophet) (4.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\m\\anaconda3\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\m\\anaconda3\\lib\\site-packages (from holidays>=0.41->neuralprophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.5.3->neuralprophet) (3.1.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.0.0->neuralprophet) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from plotly<6.0.0,>=5.13.1->neuralprophet) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (0.12.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from tensorboard<3.0.0,>=2.11.2->neuralprophet) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->neuralprophet) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\m\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m\\anaconda3\\lib\\site-packages (from tqdm->captum>=0.6.0->neuralprophet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.11.2->neuralprophet) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\m\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.863% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "WARNING - (NP.config.set_lr_finder_args) - Learning rate finder: The number of batches (46) is too small than the required number                     for the learning rate finder (229). The results might not be optimal.\n",
      "Missing logger folder: c:\\Users\\m\\Documents\\GitHub\\sda.final.project\\lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6fa684e2ea4df483fe2ca3d71b9156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµ\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralProphet()\n\u001b[1;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\u001b[39;00m\n\u001b[0;32m     31\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1062\u001b[0m, in \u001b[0;36mNeuralProphet.fit\u001b[1;34m(self, df, freq, validation_df, epochs, batch_size, learning_rate, early_stopping, minimal, metrics, progress, checkpointing, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   1063\u001b[0m         df,\n\u001b[0;32m   1064\u001b[0m         progress_bar_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(progress),\n\u001b[0;32m   1065\u001b[0m         metrics_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics),\n\u001b[0;32m   1066\u001b[0m         checkpointing_enabled\u001b[38;5;241m=\u001b[39mcheckpointing,\n\u001b[0;32m   1067\u001b[0m         continue_training\u001b[38;5;241m=\u001b[39mcontinue_training,\n\u001b[0;32m   1068\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m     df_val, _, _, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(validation_df)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:2802\u001b[0m, in \u001b[0;36mNeuralProphet._train\u001b[1;34m(self, df, df_val, progress_bar_enabled, metrics_enabled, checkpointing_enabled, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   2800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mset_lr_finder_args(dataset_size\u001b[38;5;241m=\u001b[39mdataset_size, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;66;03m# Find suitable learning rate\u001b[39;00m\n\u001b[1;32m-> 2802\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_find(\n\u001b[0;32m   2803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   2804\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mlr_finder_args,\n\u001b[0;32m   2806\u001b[0m )\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lr_finder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;66;03m# Estimate the optimat learning rate from the loss curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:267\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    264\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mauto_lr_find \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1097\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m-> 1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1394\u001b[0m             fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:122\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_find(trainer, pl_module)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:107\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m lr_find(\n\u001b[0;32m    108\u001b[0m             trainer,\n\u001b[0;32m    109\u001b[0m             pl_module,\n\u001b[0;32m    110\u001b[0m             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_lr,\n\u001b[0;32m    111\u001b[0m             max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_lr,\n\u001b[0;32m    112\u001b[0m             num_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_training_steps,\n\u001b[0;32m    113\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode,\n\u001b[0;32m    114\u001b[0m             early_stop_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_stop_threshold,\n\u001b[0;32m    115\u001b[0m             update_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attr,\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:273\u001b[0m, in \u001b[0;36mlr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    270\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Restore initial state of model\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore(ckpt_path)\n\u001b[0;32m    274\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mremove_checkpoint(ckpt_path)\n\u001b[0;32m    275\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# reset restarting flag as checkpoint restoring sets it to True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:224\u001b[0m, in \u001b[0;36mCheckpointConnector.restore\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    state-restore, in this priority:\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        checkpoint_path: Path to a PyTorch Lightning checkpoint file.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# restore module states\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:90\u001b[0m, in \u001b[0;36mCheckpointConnector.resume_start\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 90\u001b[0m     loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:359\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    358\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\plugins\\io\\torch_io.py:86\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[1;34m(self, path, map_location)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\")\n",
    "\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])  # ë‚ ì§œ ë³€í™˜\n",
    "\n",
    "# 0ì¸ ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½\n",
    "data.loc[data['ì´_ê²°ì œê¸ˆì•¡'] == 0, 'ì´_ê²°ì œê¸ˆì•¡'] = np.nan\n",
    "data.loc[data['ì´_ê²°ì œê±´ìˆ˜'] == 0, 'ì´_ê²°ì œê±´ìˆ˜'] = np.nan\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ë¡œ ì²˜ë¦¬\n",
    "def fill_missing_values(group, target):\n",
    "    df = group[['ë‚ ì§œ', target]].copy()\n",
    "    df = df.rename(columns={'ë‚ ì§œ': 'ds', target: 'y'})\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„°ì™€ ì˜ˆì¸¡í•  ë°ì´í„° ë¶„ë¦¬\n",
    "    train_df = df.dropna()\n",
    "    predict_df = df[df['y'].isna()]\n",
    "    \n",
    "    if train_df.empty or predict_df.empty:\n",
    "        return group  # í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model = NeuralProphet()\n",
    "    model.fit(train_df, freq='D')\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ ì±„ìš°ê¸°\n",
    "    group.loc[group[target].isna(), target] = forecast['yhat1'].values\n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "data.to_csv(\"filled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "Missing logger folder: c:\\Users\\m\\Documents\\GitHub\\sda.final.project\\lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eb6aa3eba240f4a69b644a621a644e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\u001b[39;00m\n\u001b[0;32m     33\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 34\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\u001b[39;00m\n\u001b[0;32m     37\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mmerge(forecast[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆì œ, ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])\n",
    "\n",
    "# NeuralProphetì„ ì´ìš©í•œ ê²°ì¸¡ì¹˜ ë³´ì • í•¨ìˆ˜\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='ë‚ ì§œ')\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "    train_df = group[['ë‚ ì§œ', target]].dropna().rename(columns={'ë‚ ì§œ': 'ds', target: 'y'})\n",
    "    predict_df = group[['ë‚ ì§œ']].rename(columns={'ë‚ ì§œ': 'ds'})\n",
    "    \n",
    "    if train_df.empty:\n",
    "        return group  # ê²°ì¸¡ì¹˜ë¥¼ ì±„ìš¸ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ ë°˜í™˜\n",
    "\n",
    "    # NeuralProphet ëª¨ë¸ ì„¤ì •\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,  # ìˆ˜ë™ í•™ìŠµë¥  ì„¤ì •\n",
    "        batch_size=16,        # ë°°ì¹˜ í¬ê¸° ìˆ˜ë™ ì„¤ì •\n",
    "        daily_seasonality=True  # ì¼ë³„ ê³„ì ˆì„± í™œì„±í™”\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(train_df, freq='D', progress='none', checkpointing=False)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\n",
    "    group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "    group[target] = group[target].fillna(group['yhat1'])\n",
    "    group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"ì²˜ë¦¬ëœ ë°ì´í„°ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´_ê²°ì œê¸ˆì•¡ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\n",
      "            ds          y\n",
      "15  2020-04-28  106318760\n",
      "39  2020-04-29  177187910\n",
      "64  2020-04-30  149330922\n",
      "89  2020-05-01  150419200\n",
      "119 2020-05-02  111068980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f17b9d2d241e58815c565c6f1102e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\u001b[39;00m\n\u001b[0;32m     37\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 38\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(future)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\u001b[39;00m\n\u001b[0;32m     41\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mmerge(forecast[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆì œ, ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])\n",
    "\n",
    "# NeuralProphetì„ ì´ìš©í•œ ê²°ì¸¡ì¹˜ ë³´ì • í•¨ìˆ˜\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='ë‚ ì§œ').copy()\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "    train_df = group[['ë‚ ì§œ', target]].dropna().rename(columns={'ë‚ ì§œ': 'ds', target: 'y'}).copy()\n",
    "    predict_df = group[['ë‚ ì§œ']].rename(columns={'ë‚ ì§œ': 'ds'}).copy()\n",
    "\n",
    "    # ë°ì´í„° í™•ì¸\n",
    "    if train_df.empty:\n",
    "        print(f\"âš  {target} ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return group  # ê²°ì¸¡ì¹˜ë¥¼ ì±„ìš¸ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ ë°˜í™˜\n",
    "    \n",
    "    print(f\"âœ… {target} í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\\n{train_df.head()}\")\n",
    "\n",
    "    # NeuralProphet ëª¨ë¸ ì„¤ì •\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,  # í•™ìŠµë¥  ì„¤ì •\n",
    "        batch_size=16,        # ë°°ì¹˜ í¬ê¸° ìˆ˜ë™ ì„¤ì •\n",
    "        daily_seasonality=True  # ì¼ë³„ ê³„ì ˆì„± í™œì„±í™”\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(train_df, freq='D', progress='none', checkpointing=False)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\n",
    "    future = predict_df[['ds']].copy()\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\n",
    "    group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "    group[target] = group[target].fillna(group['yhat1'])\n",
    "    group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"ì²˜ë¦¬ëœ ë°ì´í„°ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´_ê²°ì œê¸ˆì•¡ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\n",
      "            ds          y\n",
      "15  2020-04-28  106318760\n",
      "39  2020-04-29  177187910\n",
      "64  2020-04-30  149330922\n",
      "89  2020-05-01  150419200\n",
      "119 2020-05-02  111068980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a7e7259e9f43f3bf07cf85f49c1154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.932% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ìˆ˜í–‰\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(predict_df)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m forecast\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1152\u001b[0m, in \u001b[0;36mNeuralProphet.predict\u001b[1;34m(self, df, decompose, raw)\u001b[0m\n\u001b[0;32m   1150\u001b[0m df, received_ID_col, received_single_time_series, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(df)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m-> 1152\u001b[0m df, periods_added \u001b[38;5;241m=\u001b[39m _maybe_extend_df(\n\u001b[0;32m   1153\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   1154\u001b[0m     n_forecasts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forecasts,\n\u001b[0;32m   1155\u001b[0m     max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags,\n\u001b[0;32m   1156\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq,\n\u001b[0;32m   1157\u001b[0m     config_regressors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_regressors,\n\u001b[0;32m   1158\u001b[0m     config_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_events,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[0;32m   1160\u001b[0m df \u001b[38;5;241m=\u001b[39m _prepare_dataframe_to_predict(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, df\u001b[38;5;241m=\u001b[39mdf, max_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_freq)\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:53\u001b[0m, in \u001b[0;36m_maybe_extend_df\u001b[1;34m(df, n_forecasts, max_lags, freq, config_regressors, config_events)\u001b[0m\n\u001b[0;32m     51\u001b[0m _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39minfer_frequency(df_i, n_lags\u001b[38;5;241m=\u001b[39mmax_lags, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# to get all forecasteable values with df given, maybe extend into future:\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m periods_add[df_name] \u001b[38;5;241m=\u001b[39m _get_maybe_extend_periods(\n\u001b[0;32m     54\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_i, n_forecasts\u001b[38;5;241m=\u001b[39mn_forecasts, max_lags\u001b[38;5;241m=\u001b[39mmax_lags, config_regressors\u001b[38;5;241m=\u001b[39mconfig_regressors\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m periods_add[df_name] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# This does not include future regressors or events.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# periods should be 0 if those are configured.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     last_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\data\\split.py:116\u001b[0m, in \u001b[0;36m_get_maybe_extend_periods\u001b[1;34m(df, n_forecasts, max_lags, config_regressors)\u001b[0m\n\u001b[0;32m    114\u001b[0m periods_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    115\u001b[0m nan_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m nan_at_end \u001b[38;5;129;01mand\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m nan_at_end)]:\n\u001b[0;32m    117\u001b[0m     nan_at_end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_lags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ ìˆ˜ì • í•„ìš”)\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])\n",
    "\n",
    "# NeuralProphetì„ ì´ìš©í•œ ê²°ì¸¡ì¹˜ ë³´ì • í•¨ìˆ˜\n",
    "def fill_missing_values(group, target):\n",
    "    group = group.sort_values(by='ë‚ ì§œ').copy()\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ê²°ì¸¡ì¹˜ ì œê±°)\n",
    "    train_df = group[['ë‚ ì§œ', target]].dropna().rename(columns={'ë‚ ì§œ': 'ds', target: 'y'}).copy()\n",
    "    predict_df = group[['ë‚ ì§œ']].rename(columns={'ë‚ ì§œ': 'ds'}).copy()\n",
    "\n",
    "    if train_df.empty:\n",
    "        print(f\"âš  {target} ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return group  # ê²°ì¸¡ì¹˜ë¥¼ ì±„ìš¸ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ ë°˜í™˜\n",
    "    \n",
    "    print(f\"âœ… {target} í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\\n{train_df.head()}\")\n",
    "\n",
    "    # NeuralProphet ëª¨ë¸ ì„¤ì •\n",
    "    model = NeuralProphet(\n",
    "        learning_rate=0.01,\n",
    "        batch_size=16,\n",
    "        daily_seasonality=True\n",
    "    )\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    metrics = model.fit(train_df, freq='D', progress='none', epochs=100)\n",
    "\n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    forecast = model.predict(predict_df)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\n",
    "    if 'yhat1' in forecast.columns:\n",
    "        group = group.merge(forecast[['ds', 'yhat1']], on='ds', how='left')\n",
    "        group[target] = group[target].fillna(group['yhat1'])\n",
    "        group.drop(columns=['yhat1'], inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = \"C:/Users/m/Desktop/processed_data.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"ì²˜ë¦¬ëœ ë°ì´í„°ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ds'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 'ds' ì»¬ëŸ¼ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_missing_values\u001b[39m(group, target):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    íŠ¹ì • ìì¹˜êµ¬ ê·¸ë£¹ì˜ target ì»¬ëŸ¼(ì´_ê²°ì œê¸ˆì•¡ ë˜ëŠ” ì´_ê²°ì œê±´ìˆ˜)ì˜ ê²°ì¸¡ì¹˜ë¥¼ ë³´ì •í•˜ëŠ” í•¨ìˆ˜\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ (ì˜ˆì œ)\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\")\n",
    "\n",
    "# 'ds' ì»¬ëŸ¼ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "def fill_missing_values(group, target):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ìì¹˜êµ¬ ê·¸ë£¹ì˜ target ì»¬ëŸ¼(ì´_ê²°ì œê¸ˆì•¡ ë˜ëŠ” ì´_ê²°ì œê±´ìˆ˜)ì˜ ê²°ì¸¡ì¹˜ë¥¼ ë³´ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(f\"ì²˜ë¦¬ ì¤‘ì¸ ê·¸ë£¹: {group['ìì¹˜êµ¬'].iloc[0]}\")  # ê·¸ë£¹ í™•ì¸\n",
    "    print(\"í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡:\", group.columns)  # ì»¬ëŸ¼ í™•ì¸\n",
    "\n",
    "    if target not in group.columns:\n",
    "        print(f\"âš ï¸ '{target}' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ!\")\n",
    "        return group\n",
    "    \n",
    "    # 'ds'ì™€ 'target' ì»¬ëŸ¼ëª…ì„ NeuralProphet í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "    group = group.rename(columns={target: 'y'})\n",
    "\n",
    "    # ê²°ì¸¡ê°’ì„ í¬í•¨í•œ ë°ì´í„°\n",
    "    train_df = group[['ds', 'y']].dropna()\n",
    "    predict_df = group[['ds', 'y']].copy()\n",
    "\n",
    "    print(\"ğŸ“Œ train_df ìƒ˜í”Œ:\", train_df.head())\n",
    "    print(\"ğŸ“Œ predict_df ìƒ˜í”Œ:\", predict_df.head())\n",
    "\n",
    "    if train_df.empty:\n",
    "        print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŒ. ê·¸ë£¹ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return group\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model = NeuralProphet()\n",
    "    metrics = model.fit(train_df, freq='D', progress='none', epochs=100)\n",
    "\n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    forecast = model.predict(predict_df)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜\n",
    "    if 'yhat1' in forecast.columns:\n",
    "        group.loc[group['y'].isnull(), 'y'] = forecast['yhat1']\n",
    "\n",
    "    # ì›ë˜ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°\n",
    "    group = group.rename(columns={'y': target})\n",
    "    \n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ë³´ì • ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "data.to_csv(\"processed_data.csv\", index=False)\n",
    "print(\"âœ… ê²°ì¸¡ì¹˜ ë³´ì • ì™„ë£Œ ë° ì €ì¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.863% of the data.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\df_utils.py:1152: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  converted_ds = pd.to_datetime(ds_col, utc=True).view(dtype=np.int64)\n",
      "\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 100\n",
      "WARNING - (NP.config.set_lr_finder_args) - Learning rate finder: The number of batches (46) is too small than the required number                     for the learning rate finder (229). The results might not be optimal.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b50228b3de443019296ca3649716cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê¸ˆì•¡\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìì¹˜êµ¬\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: fill_missing_values(g, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì´_ê²°ì œê±´ìˆ˜\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mfill_missing_values\u001b[1;34m(group, target)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµ\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralProphet(trainer_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[1;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\u001b[39;00m\n\u001b[0;32m     31\u001b[0m future \u001b[38;5;241m=\u001b[39m predict_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:1062\u001b[0m, in \u001b[0;36mNeuralProphet.fit\u001b[1;34m(self, df, freq, validation_df, epochs, batch_size, learning_rate, early_stopping, minimal, metrics, progress, checkpointing, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   1063\u001b[0m         df,\n\u001b[0;32m   1064\u001b[0m         progress_bar_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(progress),\n\u001b[0;32m   1065\u001b[0m         metrics_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics),\n\u001b[0;32m   1066\u001b[0m         checkpointing_enabled\u001b[38;5;241m=\u001b[39mcheckpointing,\n\u001b[0;32m   1067\u001b[0m         continue_training\u001b[38;5;241m=\u001b[39mcontinue_training,\n\u001b[0;32m   1068\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m     df_val, _, _, _ \u001b[38;5;241m=\u001b[39m df_utils\u001b[38;5;241m.\u001b[39mprep_or_copy_df(validation_df)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\neuralprophet\\forecaster.py:2802\u001b[0m, in \u001b[0;36mNeuralProphet._train\u001b[1;34m(self, df, df_val, progress_bar_enabled, metrics_enabled, checkpointing_enabled, continue_training, num_workers)\u001b[0m\n\u001b[0;32m   2800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mset_lr_finder_args(dataset_size\u001b[38;5;241m=\u001b[39mdataset_size, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;66;03m# Find suitable learning rate\u001b[39;00m\n\u001b[1;32m-> 2802\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_find(\n\u001b[0;32m   2803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   2804\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_train\u001b[38;5;241m.\u001b[39mlr_finder_args,\n\u001b[0;32m   2806\u001b[0m )\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lr_finder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;66;03m# Estimate the optimat learning rate from the loss curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\tuning.py:267\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    264\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mauto_lr_find \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1097\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m-> 1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1394\u001b[0m             fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:122\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_find(trainer, pl_module)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\lr_finder.py:107\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m lr_find(\n\u001b[0;32m    108\u001b[0m             trainer,\n\u001b[0;32m    109\u001b[0m             pl_module,\n\u001b[0;32m    110\u001b[0m             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_lr,\n\u001b[0;32m    111\u001b[0m             max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_lr,\n\u001b[0;32m    112\u001b[0m             num_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_training_steps,\n\u001b[0;32m    113\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode,\n\u001b[0;32m    114\u001b[0m             early_stop_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_stop_threshold,\n\u001b[0;32m    115\u001b[0m             update_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attr,\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\tuner\\lr_finder.py:273\u001b[0m, in \u001b[0;36mlr_find\u001b[1;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[0;32m    270\u001b[0m         log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Restore initial state of model\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore(ckpt_path)\n\u001b[0;32m    274\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mremove_checkpoint(ckpt_path)\n\u001b[0;32m    275\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# reset restarting flag as checkpoint restoring sets it to True\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:224\u001b[0m, in \u001b[0;36mCheckpointConnector.restore\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to restore everything at once from a 'PyTorch-Lightning checkpoint' file through file-read and\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    state-restore, in this priority:\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m        checkpoint_path: Path to a PyTorch Lightning checkpoint file.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# restore module states\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:90\u001b[0m, in \u001b[0;36mCheckpointConnector.resume_start\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m rank_zero_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring states from the checkpoint path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 90\u001b[0m     loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(loaded_checkpoint, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:359\u001b[0m, in \u001b[0;36mStrategy.load_checkpoint\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: _PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    358\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_io\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\plugins\\io\\torch_io.py:86\u001b[0m, in \u001b[0;36mTorchCheckpointIO.load_checkpoint\u001b[1;34m(self, path, map_location)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Aborting training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl_load(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\m\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL neuralprophet.configure.ConfigSeasonality was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ConfigSeasonality])` or the `torch.serialization.safe_globals([ConfigSeasonality])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "data = pd.read_csv(r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\")\n",
    "\n",
    "data['ë‚ ì§œ'] = pd.to_datetime(data['ë‚ ì§œ'])  # ë‚ ì§œ ë³€í™˜\n",
    "\n",
    "# 0ì¸ ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½\n",
    "data.loc[data['ì´_ê²°ì œê¸ˆì•¡'] == 0, 'ì´_ê²°ì œê¸ˆì•¡'] = np.nan\n",
    "data.loc[data['ì´_ê²°ì œê±´ìˆ˜'] == 0, 'ì´_ê²°ì œê±´ìˆ˜'] = np.nan\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ë¡œ ì²˜ë¦¬\n",
    "def fill_missing_values(group, target):\n",
    "    df = group[['ë‚ ì§œ', target]].copy()\n",
    "    df = df.rename(columns={'ë‚ ì§œ': 'ds', target: 'y'})\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„°ì™€ ì˜ˆì¸¡í•  ë°ì´í„° ë¶„ë¦¬\n",
    "    train_df = df.dropna()\n",
    "    predict_df = df[df['y'].isna()]\n",
    "    \n",
    "    if train_df.empty or predict_df.empty:\n",
    "        return group  # í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model = NeuralProphet()\n",
    "    model.fit(train_df, freq='D')\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\n",
    "    future = predict_df[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ ì±„ìš°ê¸°\n",
    "    group.loc[group[target].isna(), target] = forecast['yhat1'].values\n",
    "    return group\n",
    "\n",
    "# ëª¨ë“  ìì¹˜êµ¬ì— ì ìš©\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê¸ˆì•¡'))\n",
    "data = data.groupby('ìì¹˜êµ¬', group_keys=False).apply(lambda g: fill_missing_values(g, 'ì´_ê²°ì œê±´ìˆ˜'))\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "data.to_csv(\"filled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_8304\\473471435.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data.groupby(['ìì¹˜êµ¬', 'ìš”ì¼'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_8304\\473471435.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data.groupby(['ìì¹˜êµ¬', 'ìš”ì¼'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³´ê°„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_ë³´ê°„.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_null ì œê±°.csv\"\n",
    "data = pd.read_csv(file_path, parse_dates=['ë‚ ì§œ'])\n",
    "\n",
    "# ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (0: ì›”ìš”ì¼ ~ 6: ì¼ìš”ì¼)\n",
    "data['ìš”ì¼'] = data['ë‚ ì§œ'].dt.dayofweek\n",
    "\n",
    "# ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼\n",
    "target_cols = ['ì´_ê²°ì œê¸ˆì•¡', 'ì´_ê²°ì œê±´ìˆ˜']\n",
    "\n",
    "# 0ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "data[target_cols] = data[target_cols].replace(0, pd.NA)\n",
    "\n",
    "# ìì¹˜êµ¬ & ìš”ì¼ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë³´ê°„\n",
    "for col in target_cols:\n",
    "    data[col] = data.groupby(['ìì¹˜êµ¬', 'ìš”ì¼'])[col].transform(lambda g: g.ffill().bfill().interpolate())\n",
    "\n",
    "# ìš”ì¼ ì»¬ëŸ¼ ì‚­ì œ\n",
    "data.drop(columns=['ìš”ì¼'], inplace=True)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = r\"C:\\Users\\m\\Desktop\\ë¨¸ì‹ ëŸ¬ë‹ ì‚¬ìš© ë°ì´í„°\\ë¨¸ì‹ ëŸ¬ë‹ìš©_ì‹œê³„ì—´ë°ì´í„°_ë³´ê°„.csv\"\n",
    "data.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"ë³´ê°„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
